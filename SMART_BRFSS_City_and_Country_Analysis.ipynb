{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import randint \n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz, plot_tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, make_scorer, classification_report, RocCurveDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, SMOTEN, SMOTENC, BorderlineSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from itertools import product\n",
    "from scipy.stats import randint\n",
    "from sklearn import tree\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "SEED = 42"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Data Overview and Exploration***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data = pd.read_csv('MMSA_2021.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Some columns have many null values\n",
    "isna_data = data.isna().sum()\n",
    "for column, count in isna_data.items():\n",
    "    print(f\"{column}: {count}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# If we were to drop all null values, the data would have no records left\n",
    "data.dropna().shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand-pick Variables\n",
    "Select some interesting columns from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "columns = [\n",
    "    # === List with all meaningful columns ===\n",
    "    [\n",
    "        # Sec 1 Health Status\n",
    "        '_RFHLTH',\n",
    "\n",
    "        # Sec 2 Healthy Days\n",
    "        '_PHYS14D',\n",
    "        '_MENT14D',\n",
    "\n",
    "        # Sec 3 Health Care Access\n",
    "        '_HLTHPLN',\n",
    "        '_HCVU652',\n",
    "\n",
    "        # Sec 4 Exercise\n",
    "        '_TOTINDA',\n",
    "\n",
    "        # Sec 5 Hypertension Awareness\n",
    "        '_RFHYPE6',\n",
    "\n",
    "        # Sec 6 Cholesterol Awareness\n",
    "        '_CHOLCH3',\n",
    "        '_RFCHOL3',\n",
    "\n",
    "        # Sec 7 Chronic Health Conditions\n",
    "        '_MICHD',\n",
    "\n",
    "        # Sec 8 Arthritis\n",
    "        '_DRDXAR3',\n",
    "        '_LMTACT3',\n",
    "        '_LMTWRK3',\n",
    "\n",
    "        # Sec 11 Tobacco Use\n",
    "        '_SMOKER3',\n",
    "        '_RFSMOK3',\n",
    "        '_CURECI1',\n",
    "\n",
    "        # Sec 12 Alcohol Consumption\n",
    "        'DRNKANY5',\n",
    "        'ALCDAY5',\n",
    "        '_RFBING5',\n",
    "        '_DRNKWK1',\n",
    "        '_RFDRHV7',\n",
    "\n",
    "        # Sec 14 HIV/AIDS\n",
    "        '_AIDTST4',\n",
    "\n",
    "        # Sec 15 Fruits & Vegetables\n",
    "        'FTJUDA2_',\n",
    "        'FRUTDA2_',\n",
    "        'GRENDA1_',\n",
    "        'FRNCHDA_',\n",
    "        'POTADA1_',\n",
    "        'VEGEDA2_',\n",
    "        '_FRUTSU1',\n",
    "        '_VEGESU1',\n",
    "        '_FRTLT1A',\n",
    "        '_VEGLT1A',\n",
    "        \n",
    "        # Sec 9 Demographics\n",
    "        '_INCOMG1',\n",
    "        '_EDUCAG',\n",
    "        'CHILDREN',\n",
    "        '_RFBMI5',\n",
    "        '_BMI5CAT',\n",
    "        '_BMI5',\n",
    "        'WTKG3',\n",
    "        'HEIGHT3',\n",
    "        '_AGE_G',\n",
    "        '_AGE80',\n",
    "        '_AGE65YR',\n",
    "        '_AGEG5YR',\n",
    "        '_SEX',\n",
    "        '_RACEPRV',\n",
    "        '_RACEGR3',\n",
    "        '_RACEG21',\n",
    "        '_RACE',\n",
    "        '_HISPANC',\n",
    "        '_MRACE1',\n",
    "        '_PRACE1'\n",
    "    ],\n",
    "\n",
    "    # === 1st subset of all the meaningful columns === \n",
    "    [\n",
    "        # Sec 1 Health Status\n",
    "        '_RFHLTH',\n",
    "\n",
    "        # Sec 2 Healthy Days\n",
    "        '_PHYS14D',\n",
    "        '_MENT14D',\n",
    "\n",
    "        # Sec 3 Health Care Access\n",
    "        '_HLTHPLN',\n",
    "        '_HCVU652',\n",
    "\n",
    "        # Sec 4 Exercise\n",
    "        '_TOTINDA',\n",
    "\n",
    "        # Sec 5 Hypertension Awareness\n",
    "        '_RFHYPE6',\n",
    "\n",
    "        # Sec 6 Cholesterol Awareness\n",
    "        '_CHOLCH3',\n",
    "        '_RFCHOL3',\n",
    "\n",
    "        # Sec 7 Chronic Health Conditions\n",
    "        '_MICHD',\n",
    "\n",
    "        # Sec 8 Arthritis\n",
    "\n",
    "        # Sec 11 Tobacco Use\n",
    "        '_SMOKER3',\n",
    "\n",
    "        # Sec 12 Alcohol Consumption\n",
    "        '_RFBING5',\n",
    "        '_RFDRHV7',\n",
    "\n",
    "        # Sec 14 HIV/AIDS\n",
    "        '_AIDTST4',\n",
    "\n",
    "        # Sec 15 Fruits & Vegetables\n",
    "        '_FRUTSU1',\n",
    "        '_VEGESU1',\n",
    "        \n",
    "        # Sec 9 Demographics\n",
    "        '_INCOMG1',\n",
    "        '_EDUCAG',\n",
    "        '_RFBMI5',\n",
    "        '_BMI5CAT',\n",
    "        '_AGE_G',\n",
    "        '_SEX',\n",
    "        '_RACE'\n",
    "    ],\n",
    "\n",
    "\n",
    "    # === 2nd subset of all the meaningul columns ===\n",
    "    [\n",
    "        # Sec 1 Health Status\n",
    "        '_RFHLTH',\n",
    "\n",
    "        # Sec 2 Healthy Days\n",
    "        '_PHYS14D',\n",
    "        '_MENT14D',\n",
    "\n",
    "        # Sec 3 Health Care Access\n",
    "        '_HLTHPLN',\n",
    "        '_HCVU652',\n",
    "\n",
    "        # Sec 4 Exercise\n",
    "        '_TOTINDA',\n",
    "\n",
    "        # Sec 5 Hypertension Awareness\n",
    "        '_RFHYPE6',\n",
    "\n",
    "        # Sec 6 Cholesterol Awareness\n",
    "        '_CHOLCH3',\n",
    "        '_RFCHOL3',\n",
    "\n",
    "        # Sec 7 Chronic Health Conditions\n",
    "        '_MICHD',\n",
    "\n",
    "        # Sec 8 Arthritis\n",
    "        \n",
    "\n",
    "        # Sec 11 Tobacco Use\n",
    "        '_SMOKER3',\n",
    "\n",
    "        # Sec 12 Alcohol Consumption\n",
    "        '_RFBING5',\n",
    "        '_RFDRHV7',\n",
    "\n",
    "        # Sec 14 HIV/AIDS\n",
    "        '_AIDTST4',\n",
    "\n",
    "        # Sec 15 Fruits & Vegetables\n",
    "        '_FRTLT1A',\n",
    "        '_VEGLT1A',\n",
    "        \n",
    "        # Sec 9 Demographics\n",
    "        '_INCOMG1',\n",
    "        '_EDUCAG',\n",
    "        '_RFBMI5',\n",
    "        '_BMI5CAT',\n",
    "        '_AGE_G',\n",
    "        '_SEX',\n",
    "        '_RACE'\n",
    "    ]\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical and Continuous Columns\n",
    "We can use the one hot encoder to properly encode the values of each categorical variable that exhibits no apparent order."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "\n",
    "1: For categorical\n",
    "2: For continuous\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "cat_con_variables = {\n",
    "    # Sec 1 Health Status\n",
    "    '_RFHLTH': 1,\n",
    "\n",
    "    # Sec 2 Healthy Days\n",
    "    '_PHYS14D': 1,\n",
    "    '_MENT14D': 1,\n",
    "\n",
    "    # Sec 3 Health Care Access\n",
    "    '_HLTHPLN': 1,\n",
    "    '_HCVU652': 1,\n",
    "\n",
    "    # Sec 4 Exercise\n",
    "    '_TOTINDA': 1,\n",
    "\n",
    "    # Sec 5 Hypertension Awareness\n",
    "    '_RFHYPE6': 1,\n",
    "\n",
    "    # Sec 6 Cholesterol Awareness\n",
    "    '_CHOLCH3': 1,\n",
    "    '_RFCHOL3': 1,\n",
    "\n",
    "    # Sec 7 Chronic Health Conditions\n",
    "    '_MICHD': 1,\n",
    "\n",
    "    # Sec 8 Arthritis\n",
    "    '_DRDXAR3': 1,\n",
    "    '_LMTACT3': 1,\n",
    "    '_LMTWRK3': 1,\n",
    "\n",
    "    # Sec 11 Tobacco Use\n",
    "    '_SMOKER3': 1,\n",
    "    '_RFSMOK3': 1,\n",
    "    '_CURECI1': 1,\n",
    "\n",
    "    # Sec 12 Alcohol Consumption\n",
    "    'DRNKANY5': 1,\n",
    "    'ALCDAY5': 2,\n",
    "    '_RFBING5': 1,\n",
    "    '_DRNKWK1': 2,\n",
    "    '_RFDRHV7': 1,\n",
    "\n",
    "    # Sec 14 HIV/AIDS\n",
    "    '_AIDTST4': 1,\n",
    "\n",
    "    # Sec 15 Fruits & Vegetables\n",
    "    'FTJUDA2_': 2,\n",
    "    'FRUTDA2_': 2,\n",
    "    'GRENDA1_': 2,\n",
    "    'FRNCHDA_': 2,\n",
    "    'POTADA1_': 2,\n",
    "    'VEGEDA2_': 2,\n",
    "    '_FRUTSU1': 2,\n",
    "    '_VEGESU1': 2,\n",
    "    '_FRTLT1A': 1,\n",
    "    '_VEGLT1A': 1,\n",
    "    \n",
    "    # Sec 9 Demographics\n",
    "    '_INCOMG1': 1,\n",
    "    '_EDUCAG': 1,\n",
    "    'CHILDREN': 1,\n",
    "    '_RFBMI5': 1,\n",
    "    '_BMI5CAT': 1,\n",
    "    '_BMI5': 2,\n",
    "    'WTKG3': 2,\n",
    "    'HEIGHT3': 2,\n",
    "    \n",
    "    # === The age variables can remain the same because their values represent a natural ordering ===\n",
    "    '_AGE_G': 1,\n",
    "    '_AGE80': 1,\n",
    "    '_AGE65YR': 1,\n",
    "    '_AGEG5YR': 1,\n",
    "    \n",
    "    '_SEX': 1,\n",
    "    '_RACEPRV': 1,\n",
    "    '_RACEGR3': 1,\n",
    "    '_RACEG21': 1,\n",
    "    '_RACE': 1,\n",
    "    '_HISPANC': 1,\n",
    "    '_MRACE1': 1,\n",
    "    '_PRACE1': 1\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column Values that are Irrelevant\n",
    "Some columns/variables have values, such as \"Dont Know/Not Sure\", that are irrelevant to the analysis. These values are included in data mining methods but can be ignored in NLP mehtods. NLP methods handle text sequences of varying forms."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "irrel_col_values = {\n",
    "    # Sec 1 Health Status\n",
    "    '_RFHLTH': 9,\n",
    "\n",
    "    # Sec 2 Healthy Days\n",
    "    '_PHYS14D': 9,\n",
    "    '_MENT14D': 9,\n",
    "\n",
    "    # Sec 3 Health Care Access\n",
    "    '_HLTHPLN': 9,\n",
    "    '_HCVU652': 9,\n",
    "\n",
    "    # Sec 4 Exercise\n",
    "    '_TOTINDA': 9,\n",
    "\n",
    "    # Sec 5 Hypertension Awareness\n",
    "    '_RFHYPE6': 9,\n",
    "\n",
    "    # Sec 6 Cholesterol Awareness\n",
    "    '_CHOLCH3': 9,\n",
    "    '_RFCHOL3': 9,\n",
    "\n",
    "    # Sec 7 Chronic Health Conditions\n",
    "\n",
    "    # Sec 8 Arthritis\n",
    "    '_LMTACT3': 9,\n",
    "    '_LMTWRK3': 9,\n",
    "\n",
    "    # Sec 11 Tobacco Use\n",
    "    '_SMOKER3': 9,\n",
    "    '_RFSMOK3': 9,\n",
    "    '_CURECI1': 9,\n",
    "\n",
    "    # Sec 12 Alcohol Consumption\n",
    "    'DRNKANY5': 9,\n",
    "    'ALCDAY5': 900,\n",
    "    '_RFBING5': 9,\n",
    "    '_DRNKWK1': 99900,\n",
    "    '_RFDRHV7': 9,\n",
    "\n",
    "    # Sec 14 HIV/AIDS\n",
    "    '_AIDTST4': 9,\n",
    "\n",
    "    # Sec 15 Fruits & Vegetables\n",
    "    '_FRTLT1A': 9,\n",
    "    '_VEGLT1A': 9,\n",
    "    \n",
    "    # Sec 9 Demographics\n",
    "    '_INCOMG1': 9,\n",
    "    '_EDUCAG': 9,\n",
    "    'CHILDREN': 9,\n",
    "    '_RFBMI5': 9,\n",
    "    '_AGE65YR': 3,\n",
    "    '_AGEG5YR': 14,\n",
    "    '_RACEGR3': 9,\n",
    "    '_RACEG21': 9,\n",
    "    '_RACE': 9,\n",
    "    '_HISPANC': 9,\n",
    "    '_MRACE1': [77,99],\n",
    "    '_PRACE1': [77, 99]\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for column, values in irrel_col_values.items():\n",
    "    if isinstance(values, int):  \n",
    "        num_rows = len(data[data[column] == values])\n",
    "        print(f\"Number of rows with value {values} in column '{column}': {num_rows}\")\n",
    "    elif isinstance(values, list):  \n",
    "        num_rows = len(data[data[column].isin(values)])\n",
    "        print(f\"Number of rows with values in {values} in column '{column}': {num_rows}\")\n",
    "\n",
    "# Calculating the size of the dataset after removing rows with specified values\n",
    "filtered_data = data.copy() \n",
    "for column, values in irrel_col_values.items():\n",
    "    if isinstance(values, int):  \n",
    "        filtered_data = filtered_data[filtered_data[column] != values]\n",
    "    elif isinstance(values, list):  \n",
    "        filtered_data = filtered_data[~filtered_data[column].isin(values)]\n",
    "\n",
    "# Calculate the size of the filtered dataset\n",
    "filtered_size = filtered_data.shape[0]\n",
    "print(f\"Size of the dataset after removing rows with specified values: {filtered_size}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some columns described in the 2021 SMART City and Country CodeBook are derived from other columns in the dataset.\n",
    "\n",
    "- DROCDY3_ Calculated variable for drink-occasions-per-day. DROCDY3_ is derived from ALCDAY5 by dividing the ALCDAY5 variable by 7 days per week or 30 days per month.\n",
    "\n",
    "- _CHLDCNT Calculated variable for number of children in household. _CHLDCNT is derived from CHILDREN.\n",
    "\n",
    "- HTM4 Calculated variable for reported height in meters. HTM4 is derived from the variable HTIN4 by multiplying HTIN4 by 2.54 cm per in and dividing by 100 cm per meter. HTM4 is derived from HEIGHT2 metric values by dividing by 100.\n",
    "\n",
    "- HTIN4 Calculated variable for reported height in inches. HTIN4 is derived from HEIGHT3. HTIN4 is calculated by adding the foot portion of HEIGHT3 multiplied by 12, to the inch portion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate Dataframe Creation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check if the columns selected earlier exist in the dataset\n",
    "for col in columns[0]:\n",
    "    if col in data.columns:\n",
    "        print(col)\n",
    "    else:\n",
    "        print(f'{col} does not exist')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# How many null values each selected column has\n",
    "isna_data = data[columns[0]].isna().sum()\n",
    "for column, count in isna_data.items():\n",
    "    print(f\"{column}: {count}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create dataframes based on the selected columns\n",
    "# Question: Should we drop the rows with null values?\n",
    "dataframes = [\n",
    "    data[columns[0]].dropna().copy(),\n",
    "    data[columns[1]].dropna().copy(),\n",
    "    data[columns[2]].dropna().copy(),\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "object_cols = []\n",
    "for col, cat_con_var in cat_con_variables.items():\n",
    "    if cat_con_var == 1:\n",
    "        object_cols.append(col)\n",
    "\n",
    "print(f'Categorical Columns: \\n{object_cols}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for dataframe in dataframes:\n",
    "    dataframe['_MICHD'] = dataframe['_MICHD'].replace({1.0: 1, 2.0: 2})\n",
    "    dataframe['_MICHD'] = dataframe['_MICHD'].astype('int64')\n",
    "\n",
    "    # Convert categorical columns to integers\n",
    "    for column, col_type in cat_con_variables.items():\n",
    "        if col_type == 1 and (column in dataframe.columns):  # Check if the column is categorical\n",
    "            dataframe[column] = dataframe[column].astype('int64')\n",
    "\n",
    "    # Use this if you want to utilize the one hot encoder\n",
    "    # The one hot encoder will create sub-variables for each distinct value of a variable\n",
    "    #dataframe = pd.get_dummies(dataframe, columns=object_cols)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataframes[0]['_MICHD'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataframes[0].shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Print the total number of duplicates for one of the categories of variable \"_MICHD\"\n",
    "dataframes[2][dataframes[2]['_MICHD'] == 2].duplicated().sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split variables into groups of five\n",
    "variable_groups = [dataframes[2].columns[i:i+5] for i in range(0, len(dataframes[2].columns), 5)]\n",
    "\n",
    "# Plot each group of variables\n",
    "for group in variable_groups:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for idx, variable in enumerate(group):\n",
    "        plt.subplot(math.ceil(len(group)/2), 2, idx+1)\n",
    "        sns.countplot(data=dataframes[2], x=variable, hue=variable, palette='Set3', legend=False)\n",
    "        plt.title(f'Frequency of Categories in {variable}')\n",
    "        plt.xlabel('Categories')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Sample Creation\n",
    "Create a sample for one of the dataframes for later feature extraction and classification. Another method we can use to take a sample is to split the original dataframe and apply a combination of oversampling and undersampling to the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First way to create a sample: \n",
    "- Take all the samples from the minority class\n",
    "- Define a percentage of samples that you want to take and subtract the number of minority class samples; that will be the number of majority class samples"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Take all minority samples. Including duplicates\n",
    "minority_samples = dataframes[2][dataframes[2]['_MICHD'] == 1.0] \n",
    "num_minority_samples = len(minority_samples)\n",
    "\n",
    "\"\"\"\n",
    "Lets say that we want to take p% of the original dataframe as a sample. The first step we have to take is subtract the number of minority samples assuming we have already included them in our sample.\n",
    "The second step is to add the necessary number of majority samples in our sample until we reach the upper limit which is p% of samples. We can add some of the unique samples from the majority class \n",
    "to the sample or we can randomly pick some of them. Below, we add unique samples to the sample.\n",
    "\"\"\"\n",
    "num_majority_samples =  math.floor(dataframes[2].shape[0] * 0.25) - num_minority_samples\n",
    "majority_unique_samples = dataframes[2][dataframes[2]['_MICHD'] == 2.0].drop_duplicates()\n",
    "num_unique_majority_samples = len(majority_unique_samples)\n",
    "\n",
    "majority_samples = majority_unique_samples.sample(n=num_majority_samples)\n",
    "\n",
    "dataframe_sample = pd.concat([minority_samples, majority_samples])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second way to create a sample:\n",
    "- Perform stratified sampling using a percentage of samples"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Specify the fraction of samples you want to retain in the subset (e.g., 0.5 for 50%)\n",
    "subset_fraction = 0.1\n",
    "\n",
    "# Perform stratified sampling to create a smaller dataframe\n",
    "dataframe_sample = dataframes[2].groupby('_MICHD', group_keys = False).apply(lambda x: x.sample(frac = subset_fraction))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third way to create a sample:\n",
    "- Undersample the majority class so we can keep all instances of the minority class, thereby reducing the overall size of the dataset. The downside of this approach is we lose valuable instances of the majority class."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define the undersampler\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "x = dataframes[2].drop(columns=['_MICHD'])\n",
    "y = dataframes[2]['_MICHD']\n",
    "\n",
    "# Perform undersampling\n",
    "x_resampled, y_resampled = undersampler.fit_resample(x, y)\n",
    "\n",
    "# Construct the balanced DataFrame\n",
    "dataframe_sample = pd.concat([x_resampled, y_resampled], axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataframe_sample.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataframe_sample['_MICHD'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataframe_sample.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection with SVM\n",
    "Feature selection with a linear SVM is time consuming. However, we can use undersampling to take all the instances of the minority class and reduce the size of the majority class. Use this methodology only if you want to improve the results of the classification."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calc_cost(y_true, y_pred, cost_matrix):\n",
    "    conf = confusion_matrix(y_true,y_pred).T\n",
    "    return np.sum(conf * cost_matrix)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def find_best_params(estimator, params, train_set, validation_set, starting_point = 0):\n",
    "    train_x = train_set[0]\n",
    "    train_y = train_set[1]\n",
    "    test_x = validation_set[0]\n",
    "    test_y = validation_set[1]\n",
    "    \n",
    "    min_cost = np.inf\n",
    "    best_params = {}  \n",
    "    full_params_set = []\n",
    "    \n",
    "    if type(params) == dict:\n",
    "        \n",
    "        for values in product(*params.values()):\n",
    "            point = dict(zip(params.keys(), values))\n",
    "            full_params_set.append(point)\n",
    "        \n",
    "    elif type(params) == list:\n",
    "        \n",
    "        for params_subset in params:\n",
    "            for values in product(*params_subset.values()):\n",
    "                point = dict(zip(params_subset.keys(), values))\n",
    "                full_params_set.append(point)\n",
    "    \n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "    steps = len(full_params_set)\n",
    "    counter = starting_point\n",
    "    print(f\"Testing {steps} models in total.\")\n",
    "    start = time.time()\n",
    "    \n",
    "    performance_log = []\n",
    "    \n",
    "    for params in full_params_set[starting_point:]:\n",
    "\n",
    "        estimator.set_params(**params)\n",
    "        estimator.fit(train_x,train_y)\n",
    "        pred_y = estimator.predict(test_x)\n",
    "        cost_matrix = np.matrix([[0,1], [40,0]])\n",
    "        cost = calc_cost(test_y, pred_y, cost_matrix)\n",
    "\n",
    "        if cost < min_cost:\n",
    "            min_cost = cost\n",
    "            best_params = params\n",
    "        \n",
    "        log = f\"{counter}/{steps} | Cost: {cost} | Elapsed: {int((time.time()-start)*100)/100}s | {params}\"\n",
    "        performance_log.append(log)\n",
    "        print(\"________________________________________________________________________________________\")\n",
    "        print(log)\n",
    "        print(\"________________________________________________________________________________________\")\n",
    "        counter += 1\n",
    "        if counter % 10 == 0:\n",
    "            with open(\"Performance_Logs.txt\", \"w\") as f:\n",
    "                f.write(\"\\n\".join(performance_log))\n",
    "\n",
    "    return best_params"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "linear_params = {\"C\":[0.1,1.0,10.0,100.0],\"kernel\":[\"linear\"]}\n",
    "\n",
    "# If the columns are all type float, then there is no need to apply label encoding\n",
    "#LE = LabelEncoder()\n",
    "#data_encoded = dataframe_sample.apply(LE.fit_transform, axis = 1)\n",
    "\n",
    "target_column = \"_MICHD\"\n",
    "\n",
    "y = dataframe_sample[target_column]\n",
    "x = dataframe_sample.drop(columns = [target_column])\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x, y, test_size = 0.20)\n",
    "x_train,x_val,y_train,y_val = train_test_split(x_train, y_train, test_size = 0.25)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "best_params = find_best_params(SVC(), linear_params, [x_train, y_train], [x_val, y_val])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(best_params)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "classifier = SVC(**best_params)\n",
    "classifier.fit(x_train, y_train)\n",
    "weights = zip(list(data.columns),classifier.coef_.todense().data)\n",
    "weights_sorted = {k: v for k, v in sorted(weights.items(), key=lambda item: item[1])}\n",
    "with open(\"Important_Features.json\",\"w\") as f:\n",
    "    json.dump(weights_sorted,f)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection with Random Forests\n",
    "We can use the GridSearch, or the RandomizedSearch method with the Random Forest algorithm to evaluate the importance of each variable and identify the optimal features for assessing the risk of MI or CHD."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "target_column = \"_MICHD\"\n",
    "\n",
    "y = dataframes[2][target_column]\n",
    "x = dataframes[2].drop(columns = [target_column])\n",
    "\n",
    "# y = dataframe_sample[target_column]\n",
    "# x = dataframe_sample.drop(columns = [target_column])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define the cost matrix\n",
    "cost_matrix = [[0, 0.25],  # Cost of false positive \n",
    "               [1, 0]]  # Cost of false negative"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define a custom scoring function using the cost matrix\n",
    "def custom_score(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cost = np.sum(cm * cost_matrix)\n",
    "    return -cost  # Minimize cost, so negative of cost is returned"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Make the custom scoring function into a scorer object\n",
    "custom_scorer = make_scorer(custom_score, greater_is_better = False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define the parameter distributions to sample from\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_depth': [None] + list(range(10, 21)),\n",
    "    'min_samples_split': randint(2, 11)\n",
    "}\n",
    "\n",
    "# Initialize a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize RandomizedSearchCV with custom scoring\n",
    "random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_dist, n_iter=10, cv=5, \n",
    "                                   scoring=custom_scorer, n_jobs=4, random_state=42)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Initialize a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, n_jobs=4, scoring=custom_scorer)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize search, output the best parameters, and print feature importance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "random_search.fit(x, y)\n",
    "#grid_search.fit(x, y)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Get the best parameters from the random search\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Get the best estimator (Random Forest classifier with the best parameters)\n",
    "best_rf_classifier = random_search.best_estimator_\n",
    "\n",
    "# Train the best classifier on the entire dataset\n",
    "best_rf_classifier.fit(x, y)\n",
    "\n",
    "# Extract feature importance\n",
    "feature_importances = best_rf_classifier.feature_importances_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the importance of each feature if original unaltered variables are used."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Sort the features and their importance scores by importance in descending order\n",
    "sorted_features = sorted(zip(x.columns, feature_importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print all features and their importance scores in descending order\n",
    "print(\"\\nFeature Importance (Descending Order):\")\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the importance of each feature if one hot encoder is used."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "feature_importance_sum = {}\n",
    "\n",
    "for feature, importance in zip(x.columns, feature_importances):\n",
    "    feature_name = re.sub(r'^(.*?)_.+', r'\\1', feature)\n",
    "\n",
    "    if feature_name in feature_importance_sum:\n",
    "        feature_importance_sum[feature_name] += importance\n",
    "    else:\n",
    "        feature_importance_sum[feature_name] = importance\n",
    "\n",
    "sorted_feature_importance_sum = sorted(feature_importance_sum.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for feature, importance_sum in sorted_feature_importance_sum:\n",
    "    print(f\"{feature}: {importance_sum}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the top features if original unaltered variables are used."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "num_top_features = 23  # You can adjust this number based on your preference\n",
    "top_feature_indices = feature_importances.argsort()[-num_top_features:][::-1]\n",
    "top_features = x.columns[top_feature_indices]\n",
    "\n",
    "print(\"Top features:\")\n",
    "for feature in top_features:\n",
    "    print(feature)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the top features if one hot encoder is used."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "top_features = [sorted_feature_importance_sum[i][0] for i in range(13)]\n",
    "\n",
    "complete_features = []\n",
    "\n",
    "for feature in top_features:\n",
    "  for column in dataframe_sample.columns:\n",
    "      if column.startswith(feature):\n",
    "          suffix = column[len(feature):].lstrip('_')\n",
    "          if suffix != '':\n",
    "            complete_part = feature + '_' + suffix\n",
    "          else:\n",
    "            complete_part = feature\n",
    "\n",
    "          complete_features.append(complete_part)\n",
    "\n",
    "top_features = complete_features\n",
    "for complete_feat in top_features: print(complete_feat)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Classification with Data Mining Methods***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Frequency of the Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Counts for the \"Had_CHD_MI\" variable\n",
    "sns.countplot(data=dataframes[2],x='_MICHD')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<h1 style=\"text-align: center;\">Generation of Synthetic Data</h1>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<h2> Here we try different oversampling methods and for each variant we run the model and save the results </h2>"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "SMOTE_data = dataframes[2].copy()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "SMOTE_data['_MICHD'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "SMOTE_data.drop_duplicates()['_MICHD'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "target_column = \"_MICHD\"",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3 style=\"text-align: center;\"> First variant: Borderline SMOTE </h3>"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Split data before applying Borderline SMOTE"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "SMOTE_train_data, SMOTE_test_data = train_test_split(SMOTE_data, test_size=0.20, stratify=SMOTE_data['_MICHD'])",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Separate the dataframe into minority and majority classes\n",
    "minority_df = SMOTE_data[SMOTE_data['_MICHD'] == 1.0]\n",
    "majority_df = SMOTE_data[SMOTE_data['_MICHD'] == 2.0]\n",
    "\n",
    "# Split the minority class into training and testing sets\n",
    "minority_train, minority_test = train_test_split(minority_df, test_size=0.1)\n",
    "\n",
    "# Split the majority class into training and testing sets\n",
    "majority_train, majority_test = train_test_split(majority_df, test_size=0.3)\n",
    "\n",
    "# Concatenate the training and testing sets for both classes\n",
    "train_set = pd.concat([minority_train, majority_train])\n",
    "test_set = pd.concat([minority_test, majority_test])\n",
    "\n",
    "# Shuffle the rows in the testing set\n",
    "test_set = test_set.sample(frac=1)\n",
    "\n",
    "# Ensure that the testing set does not contain samples from the training set\n",
    "test_set = test_set[~test_set.index.isin(train_set.index)]\n",
    "\n",
    "# Verify the class distribution in the training and testing sets\n",
    "print(\"Training set class distribution:\")\n",
    "print(train_set['_MICHD'].value_counts())\n",
    "print(\"\\nTesting set class distribution:\")\n",
    "print(test_set['_MICHD'].value_counts())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Generate synthetic data for the original imbalanced data\n",
    "resampled_data = BorderlineSMOTE().fit_resample(train_set.drop(columns = ['_MICHD']), \n",
    "                                        train_set['_MICHD'])\n",
    "train_set = resampled_data[0]\n",
    "train_set[target_column] = resampled_data[1].to_numpy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Counts for the \"Had_CHD_MI\" variable\n",
    "sns.countplot(data=train_set,x='_MICHD')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# 0: Reported having MI or CHD\n",
    "# 1: Did not report having MI or CHD\n",
    "\n",
    "train_set['_MICHD'] = train_set['_MICHD'].map({1.0: 0, 2.0: 1})\n",
    "test_set['_MICHD'] = test_set['_MICHD'].map({1.0: 0, 2.0: 1})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Borderline SMOTE without spliting data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate synthetic data for the original imbalanced data\n",
    "resampled_data = BorderlineSMOTE().fit_resample(SMOTE_data.drop(columns = ['_MICHD']), \n",
    "                                        SMOTE_data['_MICHD'])\n",
    "SMOTE_data = resampled_data[0]\n",
    "SMOTE_data[target_column] = resampled_data[1].to_numpy()\n",
    "\n",
    "# This variable specifies the oversampling method we use. 1 is for the first method, 2 is for the second and so on. Based on this variable, the results are saved to the correct dataframe.\n",
    "method=1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Counts for the \"Had_CHD_MI\" variable\n",
    "sns.countplot(data=SMOTE_data,x='_MICHD')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 0: Reported having MI or CHD\n",
    "# 1: Did not report having MI or CHD\n",
    "\n",
    "SMOTE_data['_MICHD'] = SMOTE_data['_MICHD'].map({1.0: 0, 2.0: 1})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# In this dataframe we will keep the results for every oversampling variant and for all the methods we use. In rows, we have the metrics and in columns, we have the classifiers.\n",
    "BorderlineSMOTE_loss = pd.DataFrame(columns = ['XGBoost', 'Decision Tree', 'GaussianNB', 'Random Forests', 'KNN', 'Logistic regression'])\n",
    "BorderlineSMOTE_precision = pd.DataFrame(columns=['XGBoost', 'Decision Tree', 'GaussianNB', 'Random Forests', 'KNN', 'Logistic regression'])\n",
    "BorderlineSMOTE_recall = pd.DataFrame(columns=['XGBoost', 'Decision Tree', 'GaussianNB', 'Random Forests', 'KNN', 'Logistic regression'])\n",
    "BorderlineSMOTE_f1_score = pd.DataFrame(columns=['XGBoost', 'Decision Tree', 'GaussianNB', 'Random Forests', 'KNN', 'Logistic regression'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3 style=\"text-align: center;\"> Second variant: SMOTE-Tomek </h3>"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate synthetic data for the original imbalanced data\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "smote_tomek = SMOTETomek()\n",
    "resampled_data = smote_tomek.fit_resample(SMOTE_data.drop(columns=['_MICHD']), SMOTE_data['_MICHD'])\n",
    "\n",
    "SMOTE_data = resampled_data[0]\n",
    "SMOTE_data[target_column] = resampled_data[1].to_numpy()\n",
    "\n",
    "method=2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Counts for the \"Had_CHD_MI\" variable\n",
    "sns.countplot(data=SMOTE_data,x='_MICHD')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 0: Reported having MI or CHD\n",
    "# 1: Did not report having MI or CHD\n",
    "\n",
    "SMOTE_data['_MICHD'] = SMOTE_data['_MICHD'].map({1.0: 0, 2.0: 1})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# In this dataframe we will keep the results for every oversampling variant and for all the methods we use. In rows, we have the metrics and in columns, we have the classifiers.\n",
    "smote_tomek_loss = pd.DataFrame(columns = ['XGBoost', 'Decision Tree', 'GaussianNB', 'Random Forests', 'KNN', 'Logistic regression'])\n",
    "smote_tomek_precision = pd.DataFrame(columns=['XGBoost', 'Decision Tree', 'GaussianNB', 'Random Forests', 'KNN', 'Logistic regression'])\n",
    "smote_tomek_recall = pd.DataFrame(columns=['XGBoost', 'Decision Tree', 'GaussianNB', 'Random Forests', 'KNN', 'Logistic regression'])\n",
    "smote_tomek_f1_score = pd.DataFrame(columns=['XGBoost', 'Decision Tree', 'GaussianNB', 'Random Forests', 'KNN', 'Logistic regression'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3 style=\"text-align: center;\"> Third variant: ADASYN (Adaptive Synthetic Sampling) </h3>"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ADASYN is an extension of SMOTE that adaptively generates samples according to the local density of minority samples. It focuses more on regions where the class imbalance is more severe."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "adasyn = ADASYN()\n",
    "resampled_data = adasyn.fit_resample(SMOTE_data.drop(columns=['_MICHD']), SMOTE_data['_MICHD'])\n",
    "\n",
    "SMOTE_data = resampled_data[0]\n",
    "SMOTE_data[target_column] = resampled_data[1].to_numpy()\n",
    "\n",
    "method=3"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Counts for the \"Had_CHD_MI\" variable\n",
    "sns.countplot(data=SMOTE_data,x='_MICHD')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 0: Reported having MI or CHD\n",
    "# 1: Did not report having MI or CHD\n",
    "\n",
    "SMOTE_data['_MICHD'] = SMOTE_data['_MICHD'].map({1.0: 0, 2.0: 1})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# In this dataframe we will keep the results for every oversampling variant and for all the methods we use. In rows, we have the metrics and in columns, we have the classifiers.\n",
    "adasyn_loss = pd.DataFrame(columns = ['XGBoost', 'Decision Tree', 'GaussianNB', 'Random Forests', 'KNN', 'Logistic regression'])\n",
    "adasyn_precision = pd.DataFrame(columns=['XGBoost', 'Decision Tree', 'GaussianNB', 'Random Forests', 'KNN', 'Logistic regression'])\n",
    "adasyn_recall = pd.DataFrame(columns=['XGBoost', 'Decision Tree', 'GaussianNB', 'Random Forests', 'KNN', 'Logistic regression'])\n",
    "adasyn_f1_score = pd.DataFrame(columns=['XGBoost', 'Decision Tree', 'GaussianNB', 'Random Forests', 'KNN', 'Logistic regression'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3 style=\"text-align: center;\"> Fourth variant: SMOTE </h3>"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "resampled_data = SMOTE().fit_resample(SMOTE_data.drop(columns=['_MICHD']), SMOTE_data['_MICHD'])\n",
    "\n",
    "SMOTE_data = resampled_data[0]\n",
    "SMOTE_data[target_column] = resampled_data[1].to_numpy()\n",
    "\n",
    "method=4"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Counts for the \"Had_CHD_MI\" variable\n",
    "sns.countplot(data=SMOTE_data,x='_MICHD')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 0: Reported having MI or CHD\n",
    "# 1: Did not report having MI or CHD\n",
    "\n",
    "SMOTE_data['_MICHD'] = SMOTE_data['_MICHD'].map({1.0: 0, 2.0: 1})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# In this dataframe we will keep the results for every oversampling variant and for all the methods we use. In rows, we have the metrics and in columns, we have the classifiers.\n",
    "SMOTE_loss = pd.DataFrame(columns = ['XGBoost', 'Decision Tree', 'GaussianNB', 'Random Forests', 'KNN', 'Logistic regression'])\n",
    "SMOTE_precision = pd.DataFrame(columns=['XGBoost', 'Decision Tree', 'GaussianNB', 'Random Forests', 'KNN', 'Logistic regression'])\n",
    "SMOTE_recall = pd.DataFrame(columns=['XGBoost', 'Decision Tree', 'GaussianNB', 'Random Forests', 'KNN', 'Logistic regression'])\n",
    "SMOTE_f1_score = pd.DataFrame(columns=['XGBoost', 'Decision Tree', 'GaussianNB', 'Random Forests', 'KNN', 'Logistic regression'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define the cost matrix\n",
    "cost_matrix = [ \n",
    "    [0, 0.25],\n",
    "    [1, 0]\n",
    "] "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***XGBoost***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost for pre-split data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# # Calculate the scale_pos_weight based on the corrected cost matrix\n",
    "# scale_pos_weight = cost_matrix[1][0] / cost_matrix[0][1]\n",
    "# \n",
    "# # Create an XGBClassifier object\n",
    "# xgb_model = XGBClassifier(n_estimators = 500, learning_rate = 0.1, scale_pos_weight=scale_pos_weight)\n",
    "# \n",
    "# # Train the classifier\n",
    "# xgb_model.fit(train_set.drop(columns=['_MICHD']), train_set['_MICHD'])\n",
    "# \n",
    "# # Predict the response for test dataset\n",
    "# y_pred = xgb_model.predict(test_set.drop(columns=['_MICHD']))\n",
    "# \n",
    "# # Print the confusion matrix\n",
    "# print('Confusion Matrix:')\n",
    "# print(confusion_matrix(test_set['_MICHD'], y_pred))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost for non-split data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Split the balanced dataset to a set of training and testing samples \n",
    "x_train, x_test, y_train, y_test = train_test_split(SMOTE_data.drop(columns=['_MICHD']), SMOTE_data['_MICHD'], test_size=0.25, random_state=SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500],  # Number of trees in the forest\n",
    "    'learning_rate': [0.05, 0.1, 0.2],  # Learning rate\n",
    "    'max_depth': [3, 5, 7],  # Maximum depth of each tree\n",
    "    'min_child_weight': [1, 3, 5],  # Minimum sum of instance weight needed in a child\n",
    "}\n",
    "\n",
    "# Create an XGBClassifier object\n",
    "xgb_model = XGBClassifier(random_state=SEED)\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best estimator\n",
    "best_params = grid_search.best_params_\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "\n",
    "# Train the best estimator\n",
    "best_xgb_model.fit(x_train, y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = best_xgb_model.predict(x_test)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred).T)\n",
    "\n",
    "total_cost = np.sum(confusion_matrix(y_test, y_pred).T * cost_matrix)\n",
    "print(f\"\\nTotal Cost: {total_cost}\") \n",
    "\n",
    "# Print classification report for additional evaluation metrics\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"]))\n",
    "\n",
    "# Extract macro metrics from the classification report\n",
    "metrics_dict = classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"], output_dict=True)['macro avg']\n",
    "\n",
    "if method == 1:\n",
    "    BorderlineSMOTE_loss.loc['Total Cost baseline', 'XGBoost'] = total_cost\n",
    "    BorderlineSMOTE_precision.loc['macro precision baseline', 'XGBoost'] = metrics_dict['precision']\n",
    "    BorderlineSMOTE_recall.loc['macro recall baseline', 'XGBoost'] = metrics_dict['recall']\n",
    "    BorderlineSMOTE_f1_score.loc['macro f1-score baseline', 'XGBoost'] = metrics_dict['f1-score']\n",
    "elif method == 2:\n",
    "    smote_tomek_loss.loc['Total Cost baseline', 'XGBoost'] = total_cost\n",
    "    smote_tomek_precision.loc['macro precision baseline', 'XGBoost'] = metrics_dict['precision']\n",
    "    smote_tomek_recall.loc['macro recall baseline', 'XGBoost'] = metrics_dict['recall']\n",
    "    smote_tomek_f1_score.loc['macro f1-score baseline', 'XGBoost'] = metrics_dict['f1-score']\n",
    "elif method == 3:\n",
    "    adasyn_loss.loc['Total Cost baseline', 'XGBoost'] = total_cost\n",
    "    adasyn_precision.loc['macro precision baseline', 'XGBoost'] = metrics_dict['precision']\n",
    "    adasyn_recall.loc['macro recall baseline', 'XGBoost'] = metrics_dict['recall']\n",
    "    adasyn_f1_score.loc['macro f1-score baseline', 'XGBoost'] = metrics_dict['f1-score']\n",
    "else:\n",
    "    SMOTE_loss.loc['Total Cost baseline', 'XGBoost'] = total_cost\n",
    "    SMOTE_precision.loc['macro precision baseline', 'XGBoost'] = metrics_dict['precision']\n",
    "    SMOTE_recall.loc['macro recall baseline', 'XGBoost'] = metrics_dict['recall']\n",
    "    SMOTE_f1_score.loc['macro f1-score baseline', 'XGBoost'] = metrics_dict['f1-score']"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# Get feature importances\n",
    "importances = best_xgb_model.feature_importances_\n",
    "features = x_train.columns\n",
    " \n",
    "# Sort feature importances in descending order\n",
    "indices = np.argsort(importances)[::-1]\n",
    " \n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importances (XGBoost)\")\n",
    "plt.bar(range(x_train.shape[1]), importances[indices], align=\"center\")\n",
    "plt.xticks(range(x_train.shape[1]), [features[i] for i in indices], rotation=90)\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.tight_layout()\n",
    " \n",
    "# Save the plot as an image file\n",
    "plt.savefig(\"feature_importance_plot (XGBoost).png\")\n",
    " \n",
    "# Display the plot\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import shap\n",
    "\n",
    "# Explain the model's predictions using SHAP\n",
    "explainer = shap.TreeExplainer(best_xgb_model)\n",
    "shap_values = explainer.shap_values(x_train)\n",
    " \n",
    "# Save SHAP values to a file\n",
    "np.save('shap_values.npy', shap_values)\n",
    " \n",
    "# Plot the SHAP values\n",
    "shap.summary_plot(shap_values, features=x_train, feature_names=x_train.columns, show=False)\n",
    "plt.savefig('shap_summary_plot.png', bbox_inches='tight')  # Save the plot as an image\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def predict(y_prob, cost_m):\n",
    "    t_BMR = (cost_m[1][0] - cost_m[1][1])/(cost_m[0][1] - cost_m[1][1] - cost_m[0][0] + cost_m[1][0])\n",
    "    y_pred = np.greater(y_prob[:, 1], t_BMR).astype(int)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Create an XGBClassifier object\n",
    "xgb_model = CalibratedClassifierCV(XGBClassifier(**best_params, random_state=SEED), method=\"isotonic\", cv=10)\n",
    "\n",
    "# Train the classifier\n",
    "xgb_model.fit(x_train, y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_prob = xgb_model.predict_proba(x_test)\n",
    "y_pred = predict(y_prob, cost_matrix)\n",
    "    \n",
    "# Print the confusion matrix\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred).T)\n",
    "\n",
    "total_cost = np.sum(confusion_matrix(y_test, y_pred).T * cost_matrix)\n",
    "print(f\"\\nTotal loss Cost: {total_cost}\") \n",
    "\n",
    "# Print classification report for additional evaluation metrics\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred, target_names = [\"Reported having MI or CHD\", \"Did not report having MI or CHD\"]))\n",
    "\n",
    "# Extract macro metrics from the classification report\n",
    "metrics_dict = classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"], output_dict=True)['macro avg']\n",
    "\n",
    "if method == 1:\n",
    "    BorderlineSMOTE_loss.loc['Total Cost MEC', 'XGBoost'] = total_cost\n",
    "    BorderlineSMOTE_precision.loc['macro precision MEC', 'XGBoost'] = metrics_dict['precision']\n",
    "    BorderlineSMOTE_recall.loc['macro recall MEC', 'XGBoost'] = metrics_dict['recall']\n",
    "    BorderlineSMOTE_f1_score.loc['macro f1-score MEC', 'XGBoost'] = metrics_dict['f1-score']\n",
    "elif method == 2:\n",
    "    smote_tomek_loss.loc['Total Cost MEC', 'XGBoost'] = total_cost\n",
    "    smote_tomek_precision.loc['macro precision MEC', 'XGBoost'] = metrics_dict['precision']\n",
    "    smote_tomek_recall.loc['macro recall MEC', 'XGBoost'] = metrics_dict['recall']\n",
    "    smote_tomek_f1_score.loc['macro f1-score MEC', 'XGBoost'] = metrics_dict['f1-score']\n",
    "elif method == 3:\n",
    "    adasyn_loss.loc['Total Cost MEC', 'XGBoost'] = total_cost\n",
    "    adasyn_precision.loc['macro precision MEC', 'XGBoost'] = metrics_dict['precision']\n",
    "    adasyn_recall.loc['macro recall MEC', 'XGBoost'] = metrics_dict['recall']\n",
    "    adasyn_f1_score.loc['macro f1-score MEC', 'XGBoost'] = metrics_dict['f1-score']\n",
    "else:\n",
    "    SMOTE_loss.loc['Total Cost MEC', 'XGBoost'] = total_cost\n",
    "    SMOTE_precision.loc['macro precision MEC', 'XGBoost'] = metrics_dict['precision']\n",
    "    SMOTE_recall.loc['macro recall MEC', 'XGBoost'] = metrics_dict['recall']\n",
    "    SMOTE_f1_score.loc['macro f1-score MEC', 'XGBoost'] = metrics_dict['f1-score']"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate the scale_pos_weight based on the corrected cost matrix\n",
    "scale_pos_weight = cost_matrix[1][0] / cost_matrix[0][1]\n",
    "\n",
    "# Create an XGBClassifier object\n",
    "xgb_model = XGBClassifier(**best_params, scale_pos_weight=scale_pos_weight, random_state=SEED)\n",
    "\n",
    "# Train the classifier\n",
    "xgb_model.fit(x_train, y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = xgb_model.predict(x_test)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred).T)\n",
    "\n",
    "total_cost = np.sum(confusion_matrix(y_test, y_pred).T * cost_matrix)\n",
    "print(f\"\\nTotal loss Cost: {total_cost}\") \n",
    "\n",
    "# Print classification report for additional evaluation metrics\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred, target_names = [\"Reported having MI or CHD\", \"Did not report having MI or CHD\"]))\n",
    "\n",
    "# Extract macro metrics from the classification report\n",
    "metrics_dict = classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"], output_dict=True)['macro avg']\n",
    "\n",
    "if method == 1:\n",
    "    BorderlineSMOTE_loss.loc['Total Cost weighting', 'XGBoost'] = total_cost\n",
    "    BorderlineSMOTE_precision.loc['macro precision weighting', 'XGBoost'] = metrics_dict['precision']\n",
    "    BorderlineSMOTE_recall.loc['macro recall weighting', 'XGBoost'] = metrics_dict['recall']\n",
    "    BorderlineSMOTE_f1_score.loc['macro f1-score weighting', 'XGBoost'] = metrics_dict['f1-score']\n",
    "elif method == 2:\n",
    "    smote_tomek_loss.loc['Total Cost weighting', 'XGBoost'] = total_cost\n",
    "    smote_tomek_precision.loc['macro precision weighting', 'XGBoost'] = metrics_dict['precision']\n",
    "    smote_tomek_recall.loc['macro recall weighting', 'XGBoost'] = metrics_dict['recall']\n",
    "    smote_tomek_f1_score.loc['macro f1-score weighting', 'XGBoost'] = metrics_dict['f1-score']\n",
    "elif method == 3:\n",
    "    adasyn_loss.loc['Total Cost weighting', 'XGBoost'] = total_cost\n",
    "    adasyn_precision.loc['macro precision weighting', 'XGBoost'] = metrics_dict['precision']\n",
    "    adasyn_recall.loc['macro recall weighting', 'XGBoost'] = metrics_dict['recall']\n",
    "    adasyn_f1_score.loc['macro f1-score weighting', 'XGBoost'] = metrics_dict['f1-score']\n",
    "else:\n",
    "    SMOTE_loss.loc['Total Cost weighting', 'XGBoost'] = total_cost\n",
    "    SMOTE_precision.loc['macro precision weighting', 'XGBoost'] = metrics_dict['precision']\n",
    "    SMOTE_recall.loc['macro recall weighting', 'XGBoost'] = metrics_dict['recall']\n",
    "    SMOTE_f1_score.loc['macro f1-score weighting', 'XGBoost'] = metrics_dict['f1-score']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Decision Tree Classifier***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from io import StringIO\n",
    "\n",
    "# Visualize the decision tree\n",
    "def show_decision_tree_classifier(model, features):\n",
    "  dot_data = StringIO()\n",
    "  export_graphviz(model,\n",
    "                  out_file=dot_data,\n",
    "                  filled=False,\n",
    "                  rounded=True,\n",
    "                  special_characters=False,\n",
    "                  impurity=True,\n",
    "                  precision=2,\n",
    "                  rotate=False,\n",
    "                  feature_names=features,\n",
    "                  fontname='arial',\n",
    "                  label='all',\n",
    "                  class_names=['1', '2', '3', '4', '5', '6'])\n",
    "\n",
    "  graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "  graph.write_png('DecisionTree.png')\n",
    "\n",
    "  return graph"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree for pre-split data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# # Create a Decision Tree Classifier object\n",
    "# dt = DecisionTreeClassifier()\n",
    "# \n",
    "# # Train the classifier\n",
    "# dt.fit(train_set.drop(columns=['_MICHD']), train_set['_MICHD'], sample_weight=cost_matrix)\n",
    "# \n",
    "# # Predict the response for test dataset\n",
    "# y_pred = dt.predict(test_set.drop(columns=['_MICHD']))\n",
    "# \n",
    "# # Print the confusion matrix and a classification report\n",
    "# print('Confusion Matrix:')\n",
    "# print(confusion_matrix(test_set['_MICHD'], y_pred))\n",
    "# \n",
    "# print(classification_report(test_set['_MICHD'], y_pred))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree for non-split data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Split the balanced dataset to a set of training and testing samples \n",
    "x_train, x_test, y_train, y_test = train_test_split(SMOTE_data.drop(columns=['_MICHD']), SMOTE_data['_MICHD'], test_size=0.25, random_state=SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the adjusted parameter grid\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': [None, 'sqrt', 'log2', 0.5, 0.7]  # Adjusted max_features values\n",
    "}\n",
    "\n",
    "# Create a Decision Tree Classifier object\n",
    "dt = DecisionTreeClassifier(random_state=SEED)\n",
    "\n",
    "# Instantiate the grid search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Train the classifier with the best parameters\n",
    "best_dt = DecisionTreeClassifier(random_state=SEED, **best_params)\n",
    "best_dt.fit(x_train, y_train)\n",
    "\n",
    "# Predict the response for test dataset using the best classifier\n",
    "y_pred = best_dt.predict(x_test)\n",
    "\n",
    "# Print the confusion matrix and a classification report\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred).T)\n",
    "\n",
    "total_cost = np.sum(confusion_matrix(y_test, y_pred).T * cost_matrix)\n",
    "print(f\"\\nTotal Cost: {total_cost}\") \n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Extract macro metrics from the classification report\n",
    "metrics_dict = classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"], output_dict=True)['macro avg']\n",
    "\n",
    "if method==1:\n",
    "    BorderlineSMOTE_loss.loc['Total Cost baseline', 'Decision Tree'] = total_cost\n",
    "    BorderlineSMOTE_precision.loc['macro precision baseline', 'Decision Tree'] = metrics_dict['precision']\n",
    "    BorderlineSMOTE_recall.loc['macro recall baseline', 'Decision Tree'] = metrics_dict['recall']\n",
    "    BorderlineSMOTE_f1_score.loc['macro f1-score baseline', 'Decision Tree'] = metrics_dict['f1-score']\n",
    "elif method==2:\n",
    "    smote_tomek_loss.loc['Total Cost baseline', 'Decision Tree'] = total_cost\n",
    "    smote_tomek_precision.loc['macro precision baseline', 'Decision Tree'] = metrics_dict['precision']\n",
    "    smote_tomek_recall.loc['macro recall baseline', 'Decision Tree'] = metrics_dict['recall']\n",
    "    smote_tomek_f1_score.loc['macro f1-score baseline', 'Decision Tree'] = metrics_dict['f1-score']\n",
    "elif method==3:\n",
    "    adasyn_loss.loc['Total Cost baseline', 'Decision Tree'] = total_cost\n",
    "    adasyn_precision.loc['macro precision baseline', 'Decision Tree'] = metrics_dict['precision']\n",
    "    adasyn_recall.loc['macro recall baseline', 'Decision Tree'] = metrics_dict['recall']\n",
    "    adasyn_f1_score.loc['macro f1-score baseline', 'Decision Tree'] = metrics_dict['f1-score']\n",
    "else:\n",
    "    SMOTE_loss.loc['Total Cost baseline', 'Decision Tree'] = total_cost\n",
    "    SMOTE_precision.loc['macro precision baseline', 'Decision Tree'] = metrics_dict['precision']\n",
    "    SMOTE_recall.loc['macro recall baseline', 'Decision Tree'] = metrics_dict['recall']\n",
    "    SMOTE_f1_score.loc['macro f1-score baseline', 'Decision Tree'] = metrics_dict['f1-score']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    " \n",
    "# Compute permutation feature importance\n",
    "perm_importance = permutation_importance(best_dt, x_test, y_test)\n",
    " \n",
    "# Get feature names\n",
    "feature_names = x_test.columns\n",
    " \n",
    "# Get sorted indices of feature importance\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    " \n",
    "# Plot permutation feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(sorted_idx)), perm_importance.importances_mean[sorted_idx], tick_label=feature_names[sorted_idx])\n",
    "plt.xlabel('Permutation Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Permutation Feature Importance')\n",
    " \n",
    "# Save the plot as an image file\n",
    "plt.savefig(\"feature_importance_plot (DecisionTreeClassifier).png\")\n",
    " \n",
    "# Display the plot\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import shap\n",
    " \n",
    "# Explain the model's predictions using SHAP\n",
    "explainer = shap.TreeExplainer(best_dt)\n",
    "shap_values = explainer.shap_values(x_train)\n",
    " \n",
    "# Save SHAP values to a file\n",
    "np.save('shap_values.npy', shap_values)\n",
    " \n",
    "# Plot the SHAP values\n",
    "shap.summary_plot(shap_values, features=x_train, feature_names=x_train.columns, show=False)\n",
    "plt.savefig('shap_summary_plot (dt).png', bbox_inches='tight')  # Save the plot as an image\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def predict(y_prob, cost_m):\n",
    "    t_BMR = (cost_m[1][0] - cost_m[1][1])/(cost_m[0][1] - cost_m[1][1] - cost_m[0][0] + cost_m[1][0])\n",
    "    y_pred = np.greater(y_prob[:, 1], t_BMR).astype(int)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Create a Decision Tree Classifier object\n",
    "dt = CalibratedClassifierCV(\n",
    "    DecisionTreeClassifier(random_state=SEED, **best_params),\n",
    "    method=\"isotonic\",\n",
    "    cv=10\n",
    ")\n",
    "\n",
    "# Train the classifier\n",
    "dt.fit(x_train , y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_prob = dt.predict_proba(x_test)\n",
    "y_pred = predict(y_prob, cost_matrix)\n",
    "\n",
    "# Print the confusion matrix and a classification report\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred).T)\n",
    "\n",
    "total_cost = np.sum(confusion_matrix(y_test, y_pred).T * cost_matrix)\n",
    "print(f\"\\nTotal Cost: {total_cost}\") \n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Extract macro metrics from the classification report\n",
    "metrics_dict = classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"], output_dict=True)['macro avg']\n",
    "\n",
    "if method==1:\n",
    "    BorderlineSMOTE_loss.loc['Total Cost MEC', 'Decision Tree'] = total_cost\n",
    "    BorderlineSMOTE_precision.loc['macro precision MEC', 'Decision Tree'] = metrics_dict['precision']\n",
    "    BorderlineSMOTE_recall.loc['macro recall MEC', 'Decision Tree'] = metrics_dict['recall']\n",
    "    BorderlineSMOTE_f1_score.loc['macro f1-score MEC', 'Decision Tree'] = metrics_dict['f1-score']\n",
    "elif method==2:\n",
    "    smote_tomek_loss.loc['Total Cost MEC', 'Decision Tree'] = total_cost\n",
    "    smote_tomek_precision.loc['macro precision MEC', 'Decision Tree'] = metrics_dict['precision']\n",
    "    smote_tomek_recall.loc['macro recall MEC', 'Decision Tree'] = metrics_dict['recall']\n",
    "    smote_tomek_f1_score.loc['macro f1-score MEC', 'Decision Tree'] = metrics_dict['f1-score']\n",
    "elif method==3:\n",
    "    adasyn_loss.loc['Total Cost MEC', 'Decision Tree'] = total_cost\n",
    "    adasyn_precision.loc['macro precision MEC', 'Decision Tree'] = metrics_dict['precision']\n",
    "    adasyn_recall.loc['macro recall MEC', 'Decision Tree'] = metrics_dict['recall']\n",
    "    adasyn_f1_score.loc['macro f1-score MEC', 'Decision Tree'] = metrics_dict['f1-score']\n",
    "else:\n",
    "    SMOTE_loss.loc['Total Cost MEC', 'Decision Tree'] = total_cost\n",
    "    SMOTE_precision.loc['macro precision MEC', 'Decision Tree'] = metrics_dict['precision']\n",
    "    SMOTE_recall.loc['macro recall MEC', 'Decision Tree'] = metrics_dict['recall']\n",
    "    SMOTE_f1_score.loc['macro f1-score MEC', 'Decision Tree'] = metrics_dict['f1-score']"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate sample weights based on the corrected cost matrix\n",
    "sample_weights = np.zeros_like(y_train, dtype=float)\n",
    "sample_weights[y_train == 0] = cost_matrix[0][1]\n",
    "sample_weights[y_train == 1] = cost_matrix[1][0]\n",
    "\n",
    "# Create a Decision Tree Classifier object\n",
    "dt = DecisionTreeClassifier(random_state=SEED, **best_params)\n",
    "\n",
    "# Train the classifier\n",
    "dt.fit(x_train , y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = dt.predict(x_test)\n",
    "\n",
    "# Print the confusion matrix and a classification report\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred).T)\n",
    "\n",
    "total_cost = np.sum(confusion_matrix(y_test, y_pred).T * cost_matrix)\n",
    "print(f\"\\nTotal Cost: {total_cost}\") \n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Extract macro metrics from the classification report\n",
    "metrics_dict = classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"], output_dict=True)['macro avg']\n",
    "\n",
    "if method==1:\n",
    "    BorderlineSMOTE_loss.loc['Total Cost weighting', 'Decision Tree'] = total_cost\n",
    "    BorderlineSMOTE_precision.loc['macro precision weighting', 'Decision Tree'] = metrics_dict['precision']\n",
    "    BorderlineSMOTE_recall.loc['macro recall weighting', 'Decision Tree'] = metrics_dict['recall']\n",
    "    BorderlineSMOTE_f1_score.loc['macro f1-score weighting', 'Decision Tree'] = metrics_dict['f1-score']\n",
    "elif method==2:\n",
    "    smote_tomek_loss.loc['Total Cost weighting', 'Decision Tree'] = total_cost\n",
    "    smote_tomek_precision.loc['macro precision weighting', 'Decision Tree'] = metrics_dict['precision']\n",
    "    smote_tomek_recall.loc['macro recall weighting', 'Decision Tree'] = metrics_dict['recall']\n",
    "    smote_tomek_f1_score.loc['macro f1-score weighting', 'Decision Tree'] = metrics_dict['f1-score']\n",
    "elif method==3:\n",
    "    adasyn_loss.loc['Total Cost weighting', 'Decision Tree'] = total_cost\n",
    "    adasyn_precision.loc['macro precision weighting', 'Decision Tree'] = metrics_dict['precision']\n",
    "    adasyn_recall.loc['macro recall weighting', 'Decision Tree'] = metrics_dict['recall']\n",
    "    adasyn_f1_score.loc['macro f1-score weighting', 'Decision Tree'] = metrics_dict['f1-score']\n",
    "else:\n",
    "    SMOTE_loss.loc['Total Cost weighting', 'Decision Tree'] = total_cost\n",
    "    SMOTE_precision.loc['macro precision weighting', 'Decision Tree'] = metrics_dict['precision']\n",
    "    SMOTE_recall.loc['macro recall weighting', 'Decision Tree'] = metrics_dict['recall']\n",
    "    SMOTE_f1_score.loc['macro f1-score weighting', 'Decision Tree'] = metrics_dict['f1-score']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Gaussian Naive Bayes***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB for pre-split data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# # Create a Gaussian Naive Bayes Classifier object\n",
    "# gnb = GaussianNB()\n",
    "# \n",
    "# # Train the classifier\n",
    "# gnb.fit(train_set.drop(columns=['_MICHD']), train_set['_MICHD'], sample_weight=cost_matrix)\n",
    "# \n",
    "# # Predict the response for test dataset\n",
    "# y_pred = gnb.predict(test_set.drop(columns=['_MICHD']))\n",
    "# \n",
    "# # Print the confusion matrix and a classification report\n",
    "# print('Confusion Matrix:')\n",
    "# print(confusion_matrix(test_set['_MICHD'], y_pred))\n",
    "# \n",
    "# print(classification_report(test_set['_MICHD'], y_pred))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB for non-split data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Split the balanced dataset to a set of training and testing samples \n",
    "x_train, x_test, y_train, y_test = train_test_split(SMOTE_data.drop(columns=['_MICHD']), SMOTE_data['_MICHD'], test_size=0.25, random_state=SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create a Gaussian Naive Bayes Classifier object\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Define a grid of parameters to search\n",
    "param_grid = {\n",
    "    'priors': [None, [0.5, 0.5], [0.3, 0.7], [0.7, 0.3]]  # Example priors to search\n",
    "}\n",
    "\n",
    "# Instantiate the grid search with the Gaussian Naive Bayes classifier\n",
    "grid_search = GridSearchCV(estimator=gnb, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Use the best parameters to create a new Gaussian Naive Bayes classifier\n",
    "best_gnb = GaussianNB(**best_params)\n",
    "\n",
    "# Train the classifier with the best parameters\n",
    "best_gnb.fit(x_train, y_train)\n",
    "\n",
    "# Predict the response for the test dataset\n",
    "y_pred = best_gnb.predict(x_test)\n",
    "\n",
    "# Print the confusion matrix and a classification report\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred).T)\n",
    "\n",
    "total_cost = np.sum(confusion_matrix(y_test, y_pred).T * cost_matrix)\n",
    "print(f\"\\nTotal Cost: {total_cost}\")\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"]))\n",
    "\n",
    "# Extract macro metrics from the classification report\n",
    "metrics_dict = classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"], output_dict=True)['macro avg']\n",
    "\n",
    "if method==1:\n",
    "    BorderlineSMOTE_loss.loc['Total Cost baseline', 'GaussianNB'] = total_cost\n",
    "    BorderlineSMOTE_precision.loc['macro precision baseline', 'GaussianNB'] = metrics_dict['precision']\n",
    "    BorderlineSMOTE_recall.loc['macro recall baseline', 'GaussianNB'] = metrics_dict['recall']\n",
    "    BorderlineSMOTE_f1_score.loc['macro f1-score baseline', 'GaussianNB'] = metrics_dict['f1-score']\n",
    "elif method==2:\n",
    "    smote_tomek_loss.loc['Total Cost baseline', 'GaussianNB'] = total_cost\n",
    "    smote_tomek_precision.loc['macro precision baseline', 'GaussianNB'] = metrics_dict['precision']\n",
    "    smote_tomek_recall.loc['macro recall baseline', 'GaussianNB'] = metrics_dict['recall']\n",
    "    smote_tomek_f1_score.loc['macro f1-score baseline', 'GaussianNB'] = metrics_dict['f1-score']\n",
    "elif method==3:\n",
    "    adasyn_loss.loc['Total Cost baseline', 'GaussianNB'] = total_cost\n",
    "    adasyn_precision.loc['macro precision baseline', 'GaussianNB'] = metrics_dict['precision']\n",
    "    adasyn_recall.loc['macro recall baseline', 'GaussianNB'] = metrics_dict['recall']\n",
    "    adasyn_f1_score.loc['macro f1-score baseline', 'GaussianNB'] = metrics_dict['f1-score']\n",
    "else:\n",
    "    SMOTE_loss.loc['Total Cost baseline', 'GaussianNB'] = total_cost\n",
    "    SMOTE_precision.loc['macro precision baseline', 'GaussianNB'] = metrics_dict['precision']\n",
    "    SMOTE_recall.loc['macro recall baseline', 'GaussianNB'] = metrics_dict['recall']\n",
    "    SMOTE_f1_score.loc['macro f1-score baseline', 'GaussianNB'] = metrics_dict['f1-score']"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Get SHAP values\n",
    "# explainer = shap.Explainer(best_gnb, x_train)\n",
    "# shap_values = explainer(x_test)\n",
    "# \n",
    "# # Plot and save the SHAP summary plot\n",
    "# shap.summary_plot(shap_values, features=x_test, plot_type='bar', show=False)\n",
    "# plt.savefig('shap_summary_plot (best_gnb).png')  # Save the plot as PNG file\n",
    "# plt.show()  # Display the plot"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sample_weights = np.zeros_like(y_train, dtype=float)\n",
    "sample_weights[y_train == 0] = cost_matrix[0][1]\n",
    "sample_weights[y_train == 1] = cost_matrix[1][0]\n",
    "\n",
    "# Create a Gaussian Naive Bayes Classifier object\n",
    "gnb = GaussianNB(**best_params)\n",
    "\n",
    "# Train the classifier\n",
    "gnb.fit(x_train , y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = gnb.predict(x_test)\n",
    "\n",
    "# Print the confusion matrix and a classification report\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred).T)\n",
    "\n",
    "total_cost = np.sum(confusion_matrix(y_test, y_pred).T * cost_matrix)\n",
    "print(f\"\\nTotal Cost: {total_cost}\") \n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"]))\n",
    "\n",
    "# Extract macro metrics from the classification report\n",
    "metrics_dict = classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"], output_dict=True)['macro avg']\n",
    "\n",
    "if method==1:\n",
    "    BorderlineSMOTE_loss.loc['Total Cost weighting', 'GaussianNB'] = total_cost\n",
    "    BorderlineSMOTE_precision.loc['macro precision weighting', 'GaussianNB'] = metrics_dict['precision']\n",
    "    BorderlineSMOTE_recall.loc['macro recall weighting', 'GaussianNB'] = metrics_dict['recall']\n",
    "    BorderlineSMOTE_f1_score.loc['macro f1-score weighting', 'GaussianNB'] = metrics_dict['f1-score']\n",
    "elif method==2:\n",
    "    smote_tomek_loss.loc['Total Cost weighting', 'GaussianNB'] = total_cost\n",
    "    smote_tomek_precision.loc['macro precision weighting', 'GaussianNB'] = metrics_dict['precision']\n",
    "    smote_tomek_recall.loc['macro recall weighting', 'GaussianNB'] = metrics_dict['recall']\n",
    "    smote_tomek_f1_score.loc['macro f1-score weighting', 'GaussianNB'] = metrics_dict['f1-score']\n",
    "elif method==3:\n",
    "    adasyn_loss.loc['Total Cost weighting', 'GaussianNB'] = total_cost\n",
    "    adasyn_precision.loc['macro precision weighting', 'GaussianNB'] = metrics_dict['precision']\n",
    "    adasyn_recall.loc['macro recall weighting', 'GaussianNB'] = metrics_dict['recall']\n",
    "    adasyn_f1_score.loc['macro f1-score weighting', 'GaussianNB'] = metrics_dict['f1-score']\n",
    "else:\n",
    "    SMOTE_loss.loc['Total Cost weighting', 'GaussianNB'] = total_cost\n",
    "    SMOTE_precision.loc['macro precision weighting', 'GaussianNB'] = metrics_dict['precision']\n",
    "    SMOTE_recall.loc['macro recall weighting', 'GaussianNB'] = metrics_dict['recall']\n",
    "    SMOTE_f1_score.loc['macro f1-score weighting', 'GaussianNB'] = metrics_dict['f1-score']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def predict(y_prob, cost_m):\n",
    "    t_BMR = (cost_m[1][0] - cost_m[1][1])/(cost_m[0][1] - cost_m[1][1] - cost_m[0][0] + cost_m[1][0])\n",
    "    y_pred = np.greater(y_prob[:, 1], t_BMR).astype(int)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Create a Gaussian Naive Bayes Classifier object\n",
    "gnb = CalibratedClassifierCV(GaussianNB(**best_params), method=\"isotonic\", cv=10)\n",
    "\n",
    "# Train the classifier\n",
    "gnb.fit(x_train , y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_prob = gnb.predict_proba(x_test)\n",
    "y_pred = predict(y_prob, cost_matrix)\n",
    "\n",
    "# Print the confusion matrix and a classification report\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred).T)\n",
    "\n",
    "total_cost = np.sum(confusion_matrix(y_test, y_pred).T * cost_matrix)\n",
    "print(f\"\\nTotal Cost: {total_cost}\") \n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"]))\n",
    "\n",
    "# Extract macro metrics from the classification report\n",
    "metrics_dict = classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"], output_dict=True)['macro avg']\n",
    "\n",
    "if method==1:\n",
    "    BorderlineSMOTE_loss.loc['Total Cost MEC', 'GaussianNB'] = total_cost\n",
    "    BorderlineSMOTE_precision.loc['macro precision MEC', 'GaussianNB'] = metrics_dict['precision']\n",
    "    BorderlineSMOTE_recall.loc['macro recall MEC', 'GaussianNB'] = metrics_dict['recall']\n",
    "    BorderlineSMOTE_f1_score.loc['macro f1-score MEC', 'GaussianNB'] = metrics_dict['f1-score']\n",
    "elif method==2:\n",
    "    smote_tomek_loss.loc['Total Cost MEC', 'GaussianNB'] = total_cost\n",
    "    smote_tomek_precision.loc['macro precision MEC', 'GaussianNB'] = metrics_dict['precision']\n",
    "    smote_tomek_recall.loc['macro recall MEC', 'GaussianNB'] = metrics_dict['recall']\n",
    "    smote_tomek_f1_score.loc['macro f1-score MEC', 'GaussianNB'] = metrics_dict['f1-score']\n",
    "elif method==3:\n",
    "    adasyn_loss.loc['Total Cost MEC', 'GaussianNB'] = total_cost\n",
    "    adasyn_precision.loc['macro precision MEC', 'GaussianNB'] = metrics_dict['precision']\n",
    "    adasyn_recall.loc['macro recall MEC', 'GaussianNB'] = metrics_dict['recall']\n",
    "    adasyn_f1_score.loc['macro f1-score MEC', 'GaussianNB'] = metrics_dict['f1-score']\n",
    "else:\n",
    "    SMOTE_loss.loc['Total Cost MEC', 'GaussianNB'] = total_cost\n",
    "    SMOTE_precision.loc['macro precision MEC', 'GaussianNB'] = metrics_dict['precision']\n",
    "    SMOTE_recall.loc['macro recall MEC', 'GaussianNB'] = metrics_dict['recall']\n",
    "    SMOTE_f1_score.loc['macro f1-score MEC', 'GaussianNB'] = metrics_dict['f1-score']"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Random Forests***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF for pre-split data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# # Create a Random Forests Classifier object\n",
    "# rf_classifier = RandomForestClassifier(n_estimators=393, max_depth=16)\n",
    "# \n",
    "# # Train the classifier\n",
    "# rf_classifier.fit(train_set.drop(columns=['_MICHD']), train_set['_MICHD'], sample_weight=cost_matrix)\n",
    "# \n",
    "# # Predict the response for test dataset\n",
    "# y_pred = rf_classifier.predict(test_set.drop(columns=['_MICHD']))\n",
    "# \n",
    "# # Print the confusion matrix and a classification report\n",
    "# print('Confusion Matrix:')\n",
    "# print(confusion_matrix(test_set['_MICHD'], y_pred))\n",
    "# \n",
    "# print(classification_report(test_set['_MICHD'], y_pred))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF for non-split data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Split the balanced dataset to a set of training and testing samples \n",
    "x_train, x_test, y_train, y_test = train_test_split(SMOTE_data.drop(columns=['_MICHD']), SMOTE_data['_MICHD'], test_size=0.25, random_state=SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400],  # Example values, adjust as needed\n",
    "    'max_depth': [10, 20, 30, None]  # Example values, adjust as needed\n",
    "}\n",
    "\n",
    "# Create a Random Forest Classifier object\n",
    "rf_classifier = RandomForestClassifier(random_state=SEED)\n",
    "\n",
    "# Instantiate the GridSearchCV object\n",
    "grid_search = GridSearchCV(rf_classifier, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Use the best estimator to make predictions\n",
    "best_rf_classifier = grid_search.best_estimator_\n",
    "y_pred = best_rf_classifier.predict(x_test)\n",
    "\n",
    "# Print the confusion matrix and a classification report\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred).T)\n",
    "\n",
    "total_cost = np.sum(confusion_matrix(y_test, y_pred).T * cost_matrix)\n",
    "print(f\"\\nTotal Cost: {total_cost}\")\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"]))\n",
    "\n",
    "# Extract macro metrics from the classification report\n",
    "metrics_dict = classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"], output_dict=True)['macro avg']\n",
    "\n",
    "if method==1:\n",
    "    BorderlineSMOTE_loss.loc['Total Cost baseline', 'Random Forests'] = total_cost\n",
    "    BorderlineSMOTE_precision.loc['macro precision baseline', 'Random Forests'] = metrics_dict['precision']\n",
    "    BorderlineSMOTE_recall.loc['macro recall baseline', 'Random Forests'] = metrics_dict['recall']\n",
    "    BorderlineSMOTE_f1_score.loc['macro f1-score baseline', 'Random Forests'] = metrics_dict['f1-score']\n",
    "elif method==2:\n",
    "    smote_tomek_loss.loc['Total Cost baseline', 'Random Forests'] = total_cost\n",
    "    smote_tomek_precision.loc['macro precision baseline', 'Random Forests'] = metrics_dict['precision']\n",
    "    smote_tomek_recall.loc['macro recall baseline', 'Random Forests'] = metrics_dict['recall']\n",
    "    smote_tomek_f1_score.loc['macro f1-score baseline', 'Random Forests'] = metrics_dict['f1-score']\n",
    "elif method==3:\n",
    "    adasyn_loss.loc['Total Cost baseline', 'Random Forests'] = total_cost\n",
    "    adasyn_precision.loc['macro precision baseline', 'Random Forests'] = metrics_dict['precision']\n",
    "    adasyn_recall.loc['macro recall baseline', 'Random Forests'] = metrics_dict['recall']\n",
    "    adasyn_f1_score.loc['macro f1-score baseline', 'Random Forests'] = metrics_dict['f1-score']\n",
    "else:\n",
    "    SMOTE_loss.loc['Total Cost baseline', 'Random Forests'] = total_cost\n",
    "    SMOTE_precision.loc['macro precision baseline', 'Random Forests'] = metrics_dict['precision']\n",
    "    SMOTE_recall.loc['macro recall baseline', 'Random Forests'] = metrics_dict['recall']\n",
    "    SMOTE_f1_score.loc['macro f1-score baseline', 'Random Forests'] = metrics_dict['f1-score']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Get predicted probabilities for the positive class\n",
    "y_pred_proba = best_rf_classifier.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Compute calibration curve\n",
    "prob_true, prob_pred = calibration_curve(y_test, y_pred_proba, n_bins=10)\n",
    "\n",
    "# Plot calibration curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(prob_pred, prob_true, marker='o', linestyle='-')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Ideal curve\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.title('Probability Calibration Curve')\n",
    "\n",
    "# Save the plot as an image file (e.g., PNG)\n",
    "plt.savefig('calibration_curve (SMOTE RF - baseline).png')\n",
    "\n",
    "# Show the plot (optional)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Use the best estimator to make predictions\n",
    "best_rf_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Predict probabilities for ROC curve\n",
    "y_probs = best_rf_classifier.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Compute precision-recall curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_probs)\n",
    "\n",
    "# Compute area under the curve (AUC) for precision-recall curve\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall, precision, label=f'Precision-Recall curve (AUC = {pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('precision_recall_curve (SMOTE RF - baseline).png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get predicted probabilities for the positive class\n",
    "y_prob = best_rf_classifier.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Random Guessing')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('roc_curve (SMOTE rf - baseline).png')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# # Extract feature importances from the best classifier\n",
    "# feature_importances = best_rf_classifier.feature_importances_\n",
    "# \n",
    "# # Get the names of the features\n",
    "# feature_names = x_train.columns  # Assuming x_train is a DataFrame\n",
    "# \n",
    "# # Sort feature importances in descending order\n",
    "# indices = np.argsort(feature_importances)[::-1]\n",
    "# \n",
    "# # Plot the feature importances\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.title(\"Feature Importances\")\n",
    "# plt.bar(range(x_train.shape[1]), feature_importances[indices], color=\"b\", align=\"center\")\n",
    "# plt.xticks(range(x_train.shape[1]), [feature_names[i] for i in indices], rotation=90)\n",
    "# plt.xlim([-1, x_train.shape[1]])\n",
    "# plt.tight_layout()\n",
    "# \n",
    "# # Save the plot as an image file\n",
    "# plt.savefig(\"feature_importance_plot (RandomForestClassifier).png\")\n",
    "# \n",
    "# plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def predict(y_prob, cost_m):\n",
    "    t_BMR = (cost_m[1][0] - cost_m[1][1])/(cost_m[0][1] - cost_m[1][1] - cost_m[0][0] + cost_m[1][0])\n",
    "    y_pred = np.greater(y_prob[:, 1], t_BMR).astype(int)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Best parameters found from grid search\n",
    "best_n_estimators = grid_search.best_params_['n_estimators']\n",
    "best_max_depth = grid_search.best_params_['max_depth']\n",
    "\n",
    "# Create a Random Forest Classifier object with the best parameters\n",
    "rf_classifier = CalibratedClassifierCV(\n",
    "    RandomForestClassifier(n_estimators=best_n_estimators, max_depth=best_max_depth, random_state=SEED),\n",
    "    method=\"sigmoid\", cv=10)\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(x_train, y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_prob = rf_classifier.predict_proba(x_test)\n",
    "y_pred = predict(y_prob, cost_matrix)\n",
    "\n",
    "# Print the confusion matrix and a classification report\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred).T)\n",
    "\n",
    "total_cost = np.sum(confusion_matrix(y_test, y_pred).T * cost_matrix)\n",
    "print(f\"\\nTotal Cost: {total_cost}\")\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"]))\n",
    "\n",
    "# Extract macro metrics from the classification report\n",
    "metrics_dict = classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"], output_dict=True)['macro avg']\n",
    "\n",
    "if method==1:\n",
    "    BorderlineSMOTE_loss.loc['Total Cost MEC', 'Random Forests'] = total_cost\n",
    "    BorderlineSMOTE_precision.loc['macro precision MEC', 'Random Forests'] = metrics_dict['precision']\n",
    "    BorderlineSMOTE_recall.loc['macro recall MEC', 'Random Forests'] = metrics_dict['recall']\n",
    "    BorderlineSMOTE_f1_score.loc['macro f1-score MEC', 'Random Forests'] = metrics_dict['f1-score']\n",
    "elif method==2:\n",
    "    smote_tomek_loss.loc['Total Cost MEC', 'Random Forests'] = total_cost\n",
    "    smote_tomek_precision.loc['macro precision MEC', 'Random Forests'] = metrics_dict['precision']\n",
    "    smote_tomek_recall.loc['macro recall MEC', 'Random Forests'] = metrics_dict['recall']\n",
    "    smote_tomek_f1_score.loc['macro f1-score MEC', 'Random Forests'] = metrics_dict['f1-score']\n",
    "elif method==3:\n",
    "    adasyn_loss.loc['Total Cost MEC', 'Random Forests'] = total_cost\n",
    "    adasyn_precision.loc['macro precision MEC', 'Random Forests'] = metrics_dict['precision']\n",
    "    adasyn_recall.loc['macro recall MEC', 'Random Forests'] = metrics_dict['recall']\n",
    "    adasyn_f1_score.loc['macro f1-score MEC', 'Random Forests'] = metrics_dict['f1-score']\n",
    "else:\n",
    "    SMOTE_loss.loc['Total Cost MEC', 'Random Forests'] = total_cost\n",
    "    SMOTE_precision.loc['macro precision MEC', 'Random Forests'] = metrics_dict['precision']\n",
    "    SMOTE_recall.loc['macro recall MEC', 'Random Forests'] = metrics_dict['recall']\n",
    "    SMOTE_f1_score.loc['macro f1-score MEC', 'Random Forests'] = metrics_dict['f1-score']"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sample_weights = np.zeros_like(y_train, dtype=float)\n",
    "sample_weights[y_train == 0] = cost_matrix[0][1]\n",
    "sample_weights[y_train == 1] = cost_matrix[1][0]\n",
    "\n",
    "# Best parameters found from grid search\n",
    "best_n_estimators = grid_search.best_params_['n_estimators']\n",
    "best_max_depth = grid_search.best_params_['max_depth']\n",
    "\n",
    "# Create a Random Forests Classifier object\n",
    "rf_classifier = RandomForestClassifier(n_estimators=best_n_estimators, max_depth=best_max_depth, random_state=SEED)\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(x_train , y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = rf_classifier.predict(x_test)\n",
    "\n",
    "# Print the confusion matrix and a classification report\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred).T)\n",
    "\n",
    "total_cost = np.sum(confusion_matrix(y_test, y_pred).T * cost_matrix)\n",
    "print(f\"\\nTotal Cost: {total_cost}\")\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"]))\n",
    "\n",
    "# Extract macro metrics from the classification report\n",
    "metrics_dict = classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"], output_dict=True)['macro avg']\n",
    "\n",
    "if method==1:\n",
    "    BorderlineSMOTE_loss.loc['Total Cost weighting', 'Random Forests'] = total_cost\n",
    "    BorderlineSMOTE_precision.loc['macro precision weighting', 'Random Forests'] = metrics_dict['precision']\n",
    "    BorderlineSMOTE_recall.loc['macro recall weighting', 'Random Forests'] = metrics_dict['recall']\n",
    "    BorderlineSMOTE_f1_score.loc['macro f1-score weighting', 'Random Forests'] = metrics_dict['f1-score']\n",
    "elif method==2:\n",
    "    smote_tomek_loss.loc['Total Cost weighting', 'Random Forests'] = total_cost\n",
    "    smote_tomek_precision.loc['macro precision weighting', 'Random Forests'] = metrics_dict['precision']\n",
    "    smote_tomek_recall.loc['macro recall weighting', 'Random Forests'] = metrics_dict['recall']\n",
    "    smote_tomek_f1_score.loc['macro f1-score weighting', 'Random Forests'] = metrics_dict['f1-score']\n",
    "elif method==3:\n",
    "    adasyn_loss.loc['Total Cost weighting', 'Random Forests'] = total_cost\n",
    "    adasyn_precision.loc['macro precision weighting', 'Random Forests'] = metrics_dict['precision']\n",
    "    adasyn_recall.loc['macro recall weighting', 'Random Forests'] = metrics_dict['recall']\n",
    "    adasyn_f1_score.loc['macro f1-score weighting', 'Random Forests'] = metrics_dict['f1-score']\n",
    "else:\n",
    "    SMOTE_loss.loc['Total Cost weighting', 'Random Forests'] = total_cost\n",
    "    SMOTE_precision.loc['macro precision weighting', 'Random Forests'] = metrics_dict['precision']\n",
    "    SMOTE_recall.loc['macro recall weighting', 'Random Forests'] = metrics_dict['recall']\n",
    "    SMOTE_f1_score.loc['macro f1-score weighting', 'Random Forests'] = metrics_dict['f1-score']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import shap\n",
    "# \n",
    "# # Calculate SHAP values\n",
    "# explainer = shap.TreeExplainer(best_rf_classifier)\n",
    "# shap_values = explainer.shap_values(x_train)\n",
    "# \n",
    "# # Plot SHAP summary plot\n",
    "# summary_plot = shap.summary_plot(shap_values, x_train, feature_names=feature_names)\n",
    "# \n",
    "# # Save the plot\n",
    "# summary_plot.savefig(\"shap_summary_plot.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ***K-Nearest Neighbors (KNN)***"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split the balanced dataset to a set of training and testing samples \n",
    "x_train, x_test, y_train, y_test = train_test_split(SMOTE_data.drop(columns=['_MICHD']), SMOTE_data['_MICHD'], test_size=0.25, random_state=SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define a range of k values to try\n",
    "k_values = range(1, 21)  # You can adjust this range as needed\n",
    "\n",
    "# Initialize lists to store mean cross-validation scores for each k\n",
    "mean_cv_scores = []\n",
    "\n",
    "# Iterate over each k value\n",
    "for k in k_values:\n",
    "    # Create a KNeighborsClassifier object with the current k value\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    # Perform 5-fold cross-validation and compute the mean accuracy\n",
    "    cv_scores = cross_val_score(knn_model, x_train, y_train, cv=5, scoring='accuracy')\n",
    "    mean_cv_score = np.mean(cv_scores)\n",
    "    \n",
    "    # Append the mean cross-validation score to the list\n",
    "    mean_cv_scores.append(mean_cv_score)\n",
    "\n",
    "# Find the index of the k value with the highest mean cross-validation score\n",
    "best_k_index = np.argmax(mean_cv_scores)\n",
    "\n",
    "# Get the best value of k\n",
    "best_k = k_values[best_k_index]\n",
    "\n",
    "print(f\"The best value of k is: {best_k}\")\n",
    "\n",
    "# Create a KNeighborsClassifier object with the best value of k\n",
    "best_knn_model = KNeighborsClassifier(n_neighbors=best_k)\n",
    "\n",
    "# Train the classifier\n",
    "best_knn_model.fit(x_train, y_train)\n",
    "\n",
    "# Predict the response for the test dataset\n",
    "y_pred = best_knn_model.predict(x_test)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred).T)\n",
    "\n",
    "# Calculate and print the total cost\n",
    "total_cost = np.sum(confusion_matrix(y_test, y_pred).T * cost_matrix)\n",
    "print(f\"\\nTotal Cost: {total_cost}\") \n",
    "\n",
    "# Print the classification report for additional evaluation metrics\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"]))\n",
    "\n",
    "# Extract macro metrics from the classification report\n",
    "metrics_dict = classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"], output_dict=True)['macro avg']\n",
    "\n",
    "if method==1:\n",
    "    BorderlineSMOTE_loss.loc['Total Cost baseline', 'KNN'] = total_cost\n",
    "    BorderlineSMOTE_precision.loc['macro precision baseline', 'KNN'] = metrics_dict['precision']\n",
    "    BorderlineSMOTE_recall.loc['macro recall baseline', 'KNN'] = metrics_dict['recall']\n",
    "    BorderlineSMOTE_f1_score.loc['macro f1-score baseline', 'KNN'] = metrics_dict['f1-score']\n",
    "elif method==2:\n",
    "    smote_tomek_loss.loc['Total Cost baseline', 'KNN'] = total_cost\n",
    "    smote_tomek_precision.loc['macro precision baseline', 'KNN'] = metrics_dict['precision']\n",
    "    smote_tomek_recall.loc['macro recall baseline', 'KNN'] = metrics_dict['recall']\n",
    "    smote_tomek_f1_score.loc['macro f1-score baseline', 'KNN'] = metrics_dict['f1-score']\n",
    "elif method==3:\n",
    "    adasyn_loss.loc['Total Cost baseline', 'KNN'] = total_cost\n",
    "    adasyn_precision.loc['macro precision baseline', 'KNN'] = metrics_dict['precision']\n",
    "    adasyn_recall.loc['macro recall baseline', 'KNN'] = metrics_dict['recall']\n",
    "    adasyn_f1_score.loc['macro f1-score baseline', 'KNN'] = metrics_dict['f1-score']\n",
    "else:\n",
    "    SMOTE_loss.loc['Total Cost baseline', 'KNN'] = total_cost\n",
    "    SMOTE_precision.loc['macro precision baseline', 'KNN'] = metrics_dict['precision']\n",
    "    SMOTE_recall.loc['macro recall baseline', 'KNN'] = metrics_dict['recall']\n",
    "    SMOTE_f1_score.loc['macro f1-score baseline', 'KNN'] = metrics_dict['f1-score']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def predict(y_prob, cost_m):\n",
    "    t_BMR = (cost_m[1][0] - cost_m[1][1])/(cost_m[0][1] - cost_m[1][1] - cost_m[0][0] + cost_m[1][0])\n",
    "    y_pred = np.greater(y_prob[:, 1], t_BMR).astype(int)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Create a KNeighborsClassifier object with the best value of k\n",
    "knn_model = CalibratedClassifierCV(KNeighborsClassifier(n_neighbors=best_k), method='isotonic', cv=10)\n",
    "\n",
    "# Train the classifier\n",
    "knn_model.fit(x_train, y_train)\n",
    "\n",
    "# Predict the response for the test dataset\n",
    "y_prob = knn_model.predict_proba(x_test)\n",
    "y_pred = predict(y_prob, cost_matrix)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred).T)\n",
    "\n",
    "# Calculate and print the total cost\n",
    "total_cost = np.sum(confusion_matrix(y_test, y_pred).T * cost_matrix)\n",
    "print(f\"\\nTotal Cost: {total_cost}\") \n",
    "\n",
    "# Print the classification report for additional evaluation metrics\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"]))\n",
    "\n",
    "# Extract macro metrics from the classification report\n",
    "metrics_dict = classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"], output_dict=True)['macro avg']\n",
    "\n",
    "if method==1:\n",
    "    BorderlineSMOTE_loss.loc['Total Cost MEC', 'KNN'] = total_cost\n",
    "    BorderlineSMOTE_precision.loc['macro precision MEC', 'KNN'] = metrics_dict['precision']\n",
    "    BorderlineSMOTE_recall.loc['macro recall MEC', 'KNN'] = metrics_dict['recall']\n",
    "    BorderlineSMOTE_f1_score.loc['macro f1-score MEC', 'KNN'] = metrics_dict['f1-score']\n",
    "elif method==2:\n",
    "    smote_tomek_loss.loc['Total Cost MEC', 'KNN'] = total_cost\n",
    "    smote_tomek_precision.loc['macro precision MEC', 'KNN'] = metrics_dict['precision']\n",
    "    smote_tomek_recall.loc['macro recall MEC', 'KNN'] = metrics_dict['recall']\n",
    "    smote_tomek_f1_score.loc['macro f1-score MEC', 'KNN'] = metrics_dict['f1-score']\n",
    "elif method==3:\n",
    "    adasyn_loss.loc['Total Cost MEC', 'KNN'] = total_cost\n",
    "    adasyn_precision.loc['macro precision MEC', 'KNN'] = metrics_dict['precision']\n",
    "    adasyn_recall.loc['macro recall MEC', 'KNN'] = metrics_dict['recall']\n",
    "    adasyn_f1_score.loc['macro f1-score MEC', 'KNN'] = metrics_dict['f1-score']\n",
    "else:\n",
    "    SMOTE_loss.loc['Total Cost MEC', 'KNN'] = total_cost\n",
    "    SMOTE_precision.loc['macro precision MEC', 'KNN'] = metrics_dict['precision']\n",
    "    SMOTE_recall.loc['macro recall MEC', 'KNN'] = metrics_dict['recall']\n",
    "    SMOTE_f1_score.loc['macro f1-score MEC', 'KNN'] = metrics_dict['f1-score']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# sample_weights = np.zeros_like(y_train, dtype=float)\n",
    "# sample_weights[y_train == 0] = cost_matrix[0][1]\n",
    "# sample_weights[y_train == 1] = cost_matrix[1][0]\n",
    "\n",
    "# Convert y_train to a NumPy array\n",
    "y_train_array = y_train.values\n",
    "\n",
    "# Get indices of minority class instances\n",
    "minority_indices = np.where(y_train == 1)[0]\n",
    "\n",
    "# Replicate minority class instances with their respective weights\n",
    "x_train_up = np.repeat(x_train.iloc[minority_indices], sample_weights[minority_indices].astype(int), axis=0)\n",
    "y_train_up = np.repeat(y_train_array[minority_indices], sample_weights[minority_indices].astype(int))\n",
    "\n",
    "# Concatenate the upsampled minority class with the majority class\n",
    "x_train_weighted = np.vstack((x_train[y_train == 0], x_train_up))\n",
    "y_train_weighted = np.hstack((y_train[y_train == 0], y_train_up))\n",
    "\n",
    "# Create a KNeighborsClassifier object with the best value of k\n",
    "knn_model = KNeighborsClassifier(n_neighbors=best_k)\n",
    "\n",
    "# Train the classifier\n",
    "knn_model.fit(x_train_weighted, y_train_weighted)\n",
    "\n",
    "# Predict the response for the test dataset\n",
    "y_pred = knn_model.predict(x_test)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred).T)\n",
    "\n",
    "# Calculate and print the total cost\n",
    "total_cost = np.sum(confusion_matrix(y_test, y_pred).T * cost_matrix)\n",
    "print(f\"\\nTotal Cost: {total_cost}\") \n",
    "\n",
    "# Print the classification report for additional evaluation metrics\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"]))\n",
    "\n",
    "# Extract macro metrics from the classification report\n",
    "metrics_dict = classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"], output_dict=True)['macro avg']\n",
    "\n",
    "if method==1:\n",
    "    BorderlineSMOTE_loss.loc['Total Cost weighting', 'KNN'] = total_cost\n",
    "    BorderlineSMOTE_precision.loc['macro precision weighting', 'KNN'] = metrics_dict['precision']\n",
    "    BorderlineSMOTE_recall.loc['macro recall weighting', 'KNN'] = metrics_dict['recall']\n",
    "    BorderlineSMOTE_f1_score.loc['macro f1-score weighting', 'KNN'] = metrics_dict['f1-score']\n",
    "elif method==2:\n",
    "    smote_tomek_loss.loc['Total Cost weighting', 'KNN'] = total_cost\n",
    "    smote_tomek_precision.loc['macro precision weighting', 'KNN'] = metrics_dict['precision']\n",
    "    smote_tomek_recall.loc['macro recall weighting', 'KNN'] = metrics_dict['recall']\n",
    "    smote_tomek_f1_score.loc['macro f1-score weighting', 'KNN'] = metrics_dict['f1-score']\n",
    "elif method==3:\n",
    "    adasyn_loss.loc['Total Cost weighting', 'KNN'] = total_cost\n",
    "    adasyn_precision.loc['macro precision weighting', 'KNN'] = metrics_dict['precision']\n",
    "    adasyn_recall.loc['macro recall weighting', 'KNN'] = metrics_dict['recall']\n",
    "    adasyn_f1_score.loc['macro f1-score weighting', 'KNN'] = metrics_dict['f1-score']\n",
    "else:\n",
    "    SMOTE_loss.loc['Total Cost weighting', 'KNN'] = total_cost\n",
    "    SMOTE_precision.loc['macro precision weighting', 'KNN'] = metrics_dict['precision']\n",
    "    SMOTE_recall.loc['macro recall weighting', 'KNN'] = metrics_dict['recall']\n",
    "    SMOTE_f1_score.loc['macro f1-score weighting', 'KNN'] = metrics_dict['f1-score']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ***LogisticRegression***"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],  # Regularization penalty\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Inverse of regularization strength\n",
    "    'solver': ['liblinear', 'saga'] \n",
    "}\n",
    "\n",
    "# Create a Logistic Regression Classifier object\n",
    "logreg_classifier = LogisticRegression(random_state=SEED)\n",
    "\n",
    "# Instantiate the GridSearchCV object\n",
    "grid_search = GridSearchCV(logreg_classifier, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Use the best estimator to make predictions\n",
    "best_logreg_classifier = grid_search.best_estimator_\n",
    "y_pred = best_logreg_classifier.predict(x_test)\n",
    "\n",
    "# Print the confusion matrix and a classification report\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred).T)\n",
    "\n",
    "total_cost = np.sum(confusion_matrix(y_test, y_pred).T * cost_matrix)\n",
    "print(f\"\\nTotal Cost: {total_cost}\")\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"]))\n",
    "\n",
    "# Extract macro metrics from the classification report\n",
    "metrics_dict = classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"], output_dict=True)['macro avg']\n",
    "\n",
    "if method==1:\n",
    "    BorderlineSMOTE_loss.loc['Total Cost baseline', 'Logistic Regression'] = total_cost\n",
    "    BorderlineSMOTE_precision.loc['macro precision baseline', 'Logistic Regression'] = metrics_dict['precision']\n",
    "    BorderlineSMOTE_recall.loc['macro recall baseline', 'Logistic Regression'] = metrics_dict['recall']\n",
    "    BorderlineSMOTE_f1_score.loc['macro f1-score baseline', 'Logistic Regression'] = metrics_dict['f1-score']\n",
    "elif method==2:\n",
    "    smote_tomek_loss.loc['Total Cost baseline', 'Logistic Regression'] = total_cost\n",
    "    smote_tomek_precision.loc['macro precision baseline', 'Logistic Regression'] = metrics_dict['precision']\n",
    "    smote_tomek_recall.loc['macro recall baseline', 'Logistic Regression'] = metrics_dict['recall']\n",
    "    smote_tomek_f1_score.loc['macro f1-score baseline', 'Logistic Regression'] = metrics_dict['f1-score']\n",
    "elif method==3:\n",
    "    adasyn_loss.loc['Total Cost baseline', 'Logistic Regression'] = total_cost\n",
    "    adasyn_precision.loc['macro precision baseline', 'Logistic Regression'] = metrics_dict['precision']\n",
    "    adasyn_recall.loc['macro recall baseline', 'Logistic Regression'] = metrics_dict['recall']\n",
    "    adasyn_f1_score.loc['macro f1-score baseline', 'Logistic Regression'] = metrics_dict['f1-score']\n",
    "else:\n",
    "    SMOTE_loss.loc['Total Cost baseline', 'Logistic Regression'] = total_cost\n",
    "    SMOTE_precision.loc['macro precision baseline', 'Logistic Regression'] = metrics_dict['precision']\n",
    "    SMOTE_recall.loc['macro recall baseline', 'Logistic Regression'] = metrics_dict['recall']\n",
    "    SMOTE_f1_score.loc['macro f1-score baseline', 'Logistic Regression'] = metrics_dict['f1-score']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def predict(y_prob, cost_m):\n",
    "    t_BMR = (cost_m[1][0] - cost_m[1][1])/(cost_m[0][1] - cost_m[1][1] - cost_m[0][0] + cost_m[1][0])\n",
    "    y_pred = np.greater(y_prob[:, 1], t_BMR).astype(int)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "# Create a logistic regression classifier with the best parameters\n",
    "logreg_model = CalibratedClassifierCV(LogisticRegression(**grid_search.best_params_, random_state=SEED), method='isotonic', cv=10)\n",
    "\n",
    "# Train the classifier\n",
    "logreg_model.fit(x_train, y_train)\n",
    "\n",
    "# Predict probabilities for the test dataset\n",
    "y_prob = logreg_model.predict_proba(x_test)\n",
    "\n",
    "# Use the custom predict function to get binary predictions\n",
    "y_pred = predict(y_prob, cost_matrix)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred).T)\n",
    "\n",
    "# Calculate and print the total cost\n",
    "total_cost = np.sum(confusion_matrix(y_test, y_pred).T * cost_matrix)\n",
    "print(f\"\\nTotal Cost: {total_cost}\") \n",
    "\n",
    "# Print the classification report for additional evaluation metrics\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"]))\n",
    "\n",
    "# Extract macro metrics from the classification report\n",
    "metrics_dict = classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"], output_dict=True)['macro avg']\n",
    "\n",
    "# Update the appropriate DataFrame based on the chosen method\n",
    "if method == 1:\n",
    "    BorderlineSMOTE_loss.loc['Total Cost MEC', 'Logistic Regression'] = total_cost\n",
    "    BorderlineSMOTE_precision.loc['macro precision MEC', 'Logistic Regression'] = metrics_dict['precision']\n",
    "    BorderlineSMOTE_recall.loc['macro recall MEC', 'Logistic Regression'] = metrics_dict['recall']\n",
    "    BorderlineSMOTE_f1_score.loc['macro f1-score MEC', 'Logistic Regression'] = metrics_dict['f1-score']\n",
    "elif method == 2:\n",
    "    smote_tomek_loss.loc['Total Cost MEC', 'Logistic Regression'] = total_cost\n",
    "    smote_tomek_precision.loc['macro precision MEC', 'Logistic Regression'] = metrics_dict['precision']\n",
    "    smote_tomek_recall.loc['macro recall MEC', 'Logistic Regression'] = metrics_dict['recall']\n",
    "    smote_tomek_f1_score.loc['macro f1-score MEC', 'Logistic Regression'] = metrics_dict['f1-score']\n",
    "elif method == 3:\n",
    "    adasyn_loss.loc['Total Cost MEC', 'Logistic Regression'] = total_cost\n",
    "    adasyn_precision.loc['macro precision MEC', 'Logistic Regression'] = metrics_dict['precision']\n",
    "    adasyn_recall.loc['macro recall MEC', 'Logistic Regression'] = metrics_dict['recall']\n",
    "    adasyn_f1_score.loc['macro f1-score MEC', 'Logistic Regression'] = metrics_dict['f1-score']\n",
    "else:\n",
    "    SMOTE_loss.loc['Total Cost MEC', 'Logistic Regression'] = total_cost\n",
    "    SMOTE_precision.loc['macro precision MEC', 'Logistic Regression'] = metrics_dict['precision']\n",
    "    SMOTE_recall.loc['macro recall MEC', 'Logistic Regression'] = metrics_dict['recall']\n",
    "    SMOTE_f1_score.loc['macro f1-score MEC', 'Logistic Regression'] = metrics_dict['f1-score']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sample_weights = np.zeros_like(y_train, dtype=float)\n",
    "sample_weights[y_train == 0] = cost_matrix[0][1]\n",
    "sample_weights[y_train == 1] = cost_matrix[1][0]\n",
    "\n",
    "# Create a logistic regression classifier with the best parameters\n",
    "logreg_model = LogisticRegression(**grid_search.best_params_, random_state=SEED, max_iter=10000)\n",
    "\n",
    "# Train the classifier with sample weights\n",
    "logreg_classifier.fit(x_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Predict the response for the test dataset\n",
    "y_pred = logreg_classifier.predict(x_test)\n",
    "\n",
    "# Print the confusion matrix and a classification report\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred).T)\n",
    "\n",
    "total_cost = np.sum(confusion_matrix(y_test, y_pred).T * cost_matrix)\n",
    "print(f\"\\nTotal Cost: {total_cost}\")\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"]))\n",
    "\n",
    "# Extract macro metrics from the classification report\n",
    "metrics_dict = classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"], output_dict=True)['macro avg']\n",
    "\n",
    "# Update the appropriate DataFrame based on the chosen method\n",
    "if method == 1:\n",
    "    BorderlineSMOTE_loss.loc['Total Cost weighting', 'Logistic Regression'] = total_cost\n",
    "    BorderlineSMOTE_precision.loc['macro precision weighting', 'Logistic Regression'] = metrics_dict['precision']\n",
    "    BorderlineSMOTE_recall.loc['macro recall weighting', 'Logistic Regression'] = metrics_dict['recall']\n",
    "    BorderlineSMOTE_f1_score.loc['macro f1-score weighting', 'Logistic Regression'] = metrics_dict['f1-score']\n",
    "elif method == 2:\n",
    "    smote_tomek_loss.loc['Total Cost weighting', 'Logistic Regression'] = total_cost\n",
    "    smote_tomek_precision.loc['macro precision weighting', 'Logistic Regression'] = metrics_dict['precision']\n",
    "    smote_tomek_recall.loc['macro recall weighting', 'Logistic Regression'] = metrics_dict['recall']\n",
    "    smote_tomek_f1_score.loc['macro f1-score weighting', 'Logistic Regression'] = metrics_dict['f1-score']\n",
    "elif method == 3:\n",
    "    adasyn_loss.loc['Total Cost weighting', 'Logistic Regression'] = total_cost\n",
    "    adasyn_precision.loc['macro precision weighting', 'Logistic Regression'] = metrics_dict['precision']\n",
    "    adasyn_recall.loc['macro recall weighting', 'Logistic Regression'] = metrics_dict['recall']\n",
    "    adasyn_f1_score.loc['macro f1-score weighting', 'Logistic Regression'] = metrics_dict['f1-score']\n",
    "else:\n",
    "    SMOTE_loss.loc['Total Cost weighting', 'Logistic Regression'] = total_cost\n",
    "    SMOTE_precision.loc['macro precision weighting', 'Logistic Regression'] = metrics_dict['precision']\n",
    "    SMOTE_recall.loc['macro recall weighting', 'Logistic Regression'] = metrics_dict['recall']\n",
    "    SMOTE_f1_score.loc['macro f1-score weighting', 'Logistic Regression'] = metrics_dict['f1-score']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Classification with NLP Methods***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Original Data to Text Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences from Variable Values\n",
    "Create a small sentence from each variable value according to the 2021 SMART BRFSS Survey Codebook"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sequences = {\n",
    "    # Sec 1 Health Status\n",
    "    '_RFHLTH': {\n",
    "        1: \"i think my health is good or very good\",\n",
    "        2: \"i think my health is fair or poor\"\n",
    "    },\n",
    "\n",
    "    # Sec 2 Healthy Days\n",
    "    '_PHYS14D': {\n",
    "        1: \"my health has not been good for zero days\",\n",
    "        2: \"my health has not been good for 1-13 days\",\n",
    "        3: \"my health has not been good for 14 or more days\"\n",
    "    },\n",
    "\n",
    "    '_MENT14D': {\n",
    "        1: \"my mental health has not been good for zero days\",\n",
    "        2: \"my mental health has not been good for 1-13 days\",\n",
    "        3: \"my mental health has not been good for 14 or more days\"\n",
    "    },\n",
    "\n",
    "    # Sec 3 Health Care Access\n",
    "    '_HLTHPLN': {\n",
    "        1: \"i had some form of health insurance\",\n",
    "        2: \"i didn't have some form of health insurance\"\n",
    "    },\n",
    "\n",
    "    '_HCVU652': {\n",
    "        1: \"i have some form of health insurance\",\n",
    "        2: \"i don't have some form of health insurance\"\n",
    "    },\n",
    "\n",
    "    # Sec 4 Exercise\n",
    "    '_TOTINDA': {\n",
    "        1: \"i have exercised during the past 30 days\",\n",
    "        2: \"i haven't exercised during the past 30 days\"\n",
    "    },\n",
    "\n",
    "    # Sec 5 Hypertension Awareness\n",
    "    '_RFHYPE6': {\n",
    "        1: \"i haven't been told that my blood pressure is high\",\n",
    "        2: \"i have been told that my blood pressure is high\"\n",
    "    },\n",
    "\n",
    "    # Sec 6 Cholesterol Awareness\n",
    "    '_CHOLCH3': {\n",
    "        1: \"i have had my cholesterol checked within the past five years\",\n",
    "        2: \"i haven't had my cholesterol checked within the past five years\",\n",
    "        3: \"i have never had my cholesterol checked\"\n",
    "    },\n",
    "\n",
    "    '_RFCHOL3': {\n",
    "        1: \"i have checked my cholesterol and it's not high\",\n",
    "        2: \"i have checked my cholesterol and it's high\"\n",
    "    },\n",
    "\n",
    "    # Sec 7 Chronic Health Conditions\n",
    "    \n",
    "\n",
    "    # Sec 8 Arthritis\n",
    "    \n",
    "\n",
    "    # Sec 11 Tobacco Use\n",
    "    '_SMOKER3': {\n",
    "        1: \"i smoke every day\",\n",
    "        2: \"i smoke some days\",\n",
    "        3: \"i'm a former smoker\",\n",
    "        4: \"i've never smoked\"\n",
    "    },\n",
    "\n",
    "    # Sec 12 Alcohol Consumption\n",
    "    '_RFBING5': {\n",
    "        1: \"i didn't drink in the past 30 days, but if i did, i didn't drink five or more drinks on an occasion\",\n",
    "        2: \"i did drink in the past 30 days and had five or more drinks on one or more occasions in the past month\"\n",
    "    },\n",
    "\n",
    "    '_RFDRHV7': {\n",
    "        1: \"i'm not a heavy drinker\",\n",
    "        2: \"i'm a heavy drinker\"\n",
    "    },\n",
    "\n",
    "    # Sec 14 HIV/AIDS\n",
    "    '_AIDTST4': {\n",
    "        1: \"i have been tested for HIV\",\n",
    "        2: \"i haven't been tested for HIV\"\n",
    "    },\n",
    "\n",
    "    # Sec 15 Fruits & Vegetables\n",
    "    '_FRTLT1A': {\n",
    "        1: \"i eat fruits one or more times per day\",\n",
    "        2: \"i don't eat fruits one or more times per day\"\n",
    "    },\n",
    "\n",
    "    '_VEGLT1A': {\n",
    "        1: \"i eat vegetables one or more times per day\",\n",
    "        2: \"i don't eat vegetables one or more times per day\"\n",
    "    },\n",
    "    \n",
    "    # Sec 9 Demographics\n",
    "    '_INCOMG1': {\n",
    "        1: \"my inclome is less than 15,000\",\n",
    "        2: \"my income ranges from 15,000 to 25,000\",\n",
    "        3: \"my income ranges from 25,000 to 35,000\",\n",
    "        4: \"my income ranges from 35,000 to 50,000\",\n",
    "        5: \"my income ranges from 50,000 to 100,000\",\n",
    "        6: \"my income ranges from 100,000 to 200,000\",\n",
    "        7: \"my income ranges from 200,000 and higher\"\n",
    "    },\n",
    "\n",
    "    '_EDUCAG': {\n",
    "        1: \"i didn't graduate high school\",\n",
    "        2: \"i graduated high school\",\n",
    "        3: \"i attended college\",\n",
    "        4: \"i graduated from college\"\n",
    "    },\n",
    "\n",
    "    '_RFBMI5': {\n",
    "        1: \"my BMI is less than 25.00\",\n",
    "        2: \"my BMI is greater than 25.00\"\n",
    "    },\n",
    "\n",
    "    '_BMI5CAT': {\n",
    "        1: \"i am underweight\",\n",
    "        2: \"my weight is normal\",\n",
    "        3: \"i am overweight\",\n",
    "        4: \"i am obese\"\n",
    "    },\n",
    "\n",
    "    '_AGE_G': {\n",
    "        1: \"my age is 18 to 24\",\n",
    "        2: \"my age is 25 to 34\",\n",
    "        3: \"my age is 35 to 44\",\n",
    "        4: \"my age is 45 to 54\",\n",
    "        5: \"my age is 55 to 64\",\n",
    "        6: \"my age is 65 or older\",\n",
    "    },\n",
    "\n",
    "    '_SEX': {\n",
    "        1: \"i'm a male\",\n",
    "        2: \"i'm a female\"\n",
    "    },\n",
    "\n",
    "    '_RACE': {\n",
    "        1: \"i'm white\",\n",
    "        2: \"i'm black\",\n",
    "        3: \"i'm american indian or alaskan\",\n",
    "        4: \"i'm asian\",\n",
    "        5: \"i'm native hawaiian or other pacific islander\",\n",
    "        6: \"i belong to some other race\",\n",
    "        7: \"i'm multiracial\",\n",
    "        8: \"i'm hispanic\"\n",
    "    }\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target Variable: '_MICHD': {\n",
    "        1: \"i had MI or CHD\",\n",
    "        2: \"i haven't had MI or CHD\"\n",
    "    },"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Variable Values\n",
    "In the 2021 SMART BRFSS Codebook there are numerical values that correspond to string values, which represent a short description of the variable. The description of each variable value is used to create the small sentence. Here, we use the corresponding string values which might be the optimal and most practical choice of converting the numerical values to text. The reason why is obvious; we don't have to manually write small sentences for each variable value, we hust have to map each numerical value to its corresponding string value which already exists in the 2021 SMART Codebook."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "string_var_values = {\n",
    "    # Sec 1 Health Status\n",
    "    '_RFHLTH': {\n",
    "        1: \"Good or Better Health\",\n",
    "        2: \"Fair or Poor Health\"\n",
    "    },\n",
    "\n",
    "    # Sec 2 Healthy Days\n",
    "    '_PHYS14D': {\n",
    "        1: \"Zero days when physical health not good\",\n",
    "        2: \"1-13 days when physical health not good\",\n",
    "        3: \"14+ days when physical health not good\"\n",
    "    },\n",
    "\n",
    "    '_MENT14D': {\n",
    "        1: \"Zero days when mental health not good\",\n",
    "        2: \"1-13 days when mental health not good\",\n",
    "        3: \"14+ days when mental health not good\"\n",
    "    },\n",
    "\n",
    "    # Sec 3 Health Care Access\n",
    "    '_HLTHPLN': {\n",
    "        1: \"Have some form of insurance\",\n",
    "        2: \"Do not have some form of health insurance\"\n",
    "    },\n",
    "\n",
    "    '_HCVU652': {\n",
    "        1: \"Have some form of health insurance\",\n",
    "        2: \"Do not have any form of health insurance\"\n",
    "    },\n",
    "\n",
    "    # Sec 4 Exercise\n",
    "    '_TOTINDA': {\n",
    "        1: \"Had physical activity or exercise\",\n",
    "        2: \"No physical activity or exercise in last 30 days\"\n",
    "    },\n",
    "\n",
    "    # Sec 5 Hypertension Awareness\n",
    "    '_RFHYPE6': {\n",
    "        1: \"_RFHYPE6-No\",\n",
    "        2: \"_RFHYPE6-Yes\"\n",
    "    },\n",
    "\n",
    "    # Sec 6 Cholesterol Awareness\n",
    "    '_CHOLCH3': {\n",
    "        1: \"Had cholesterol checked in past 5 years\",\n",
    "        2: \"Did not have cholesterol checked in past 5 years\",\n",
    "        3: \"Have never had cholesterol checked\"\n",
    "    },\n",
    "\n",
    "    '_RFCHOL3': {\n",
    "        1: \"_RFCHOL3-No\",\n",
    "        2: \"_RFCHOL3-Yes\"\n",
    "    },\n",
    "\n",
    "    # Sec 7 Chronic Health Conditions\n",
    "    \n",
    "\n",
    "    # Sec 8 Arthritis\n",
    "    \n",
    "\n",
    "    # Sec 11 Tobacco Use\n",
    "    '_SMOKER3': {\n",
    "        1: \"Current smoker-now smokes every day\",\n",
    "        2: \"Current smoker-now smokes some days\",\n",
    "        3: \"Former smoker\",\n",
    "        4: \"Never smoked\"\n",
    "    },\n",
    "\n",
    "    # Sec 12 Alcohol Consumption\n",
    "    '_RFBING5': {\n",
    "        1: \"_RFBING5-No\",\n",
    "        2: \"_RFBING5-Yes\"\n",
    "    },\n",
    "\n",
    "    '_RFDRHV7': {\n",
    "        1: \"_RFDRHV7-No\",\n",
    "        2: \"_RFDRHV7-Yes\"\n",
    "    },\n",
    "\n",
    "    # Sec 14 HIV/AIDS\n",
    "    '_AIDTST4': {\n",
    "        1: \"_AIDTST4-Yes\",\n",
    "        2: \"_AIDTST4-No\"\n",
    "    },\n",
    "\n",
    "    # Sec 15 Fruits & Vegetables\n",
    "    '_FRTLT1A': {\n",
    "        1: \"Consumed fruit one or more times per day\",\n",
    "        2: \"Consumed fruit less than one time per day\"\n",
    "    },\n",
    "\n",
    "    '_VEGLT1A': {\n",
    "        1: \"Consumed vegetables one or more times per day\",\n",
    "        2: \"Consumed vegetables less than one time per day\"\n",
    "    },\n",
    "    \n",
    "    # Sec 9 Demographics\n",
    "    '_INCOMG1': {\n",
    "        1: \"Less than $15,000\",\n",
    "        2: \"$15,000 to less than $25,000\",\n",
    "        3: \"$25,000 to less than $35,000\",\n",
    "        4: \"$35,000 to less than $50,000\",\n",
    "        5: \"$50,000 to less than $100,000\",\n",
    "        6: \"$100,000 to less than $200,000\",\n",
    "        7: \"$200,000 or more\"\n",
    "    },\n",
    "\n",
    "    '_EDUCAG': {\n",
    "        1: \"Did not graduate High School\",\n",
    "        2: \"Graduated High School\",\n",
    "        3: \"Attended College or Technical School\",\n",
    "        4: \"Graduated from College or Technical School\"\n",
    "    },\n",
    "\n",
    "    '_RFBMI5': {\n",
    "        1: \"_RFBMI5-No\",\n",
    "        2: \"_RFBMI5-Yes\"\n",
    "    },\n",
    "\n",
    "    '_BMI5CAT': {\n",
    "        1: \"Underweight\",\n",
    "        2: \"Normal Weight\",\n",
    "        3: \"Overweight\",\n",
    "        4: \"Obese\"\n",
    "    },\n",
    "\n",
    "    '_AGE_G': {\n",
    "        1: \"my age is 18 to 24\",\n",
    "        2: \"my age is 25 to 34\",\n",
    "        3: \"my age is 35 to 44\",\n",
    "        4: \"my age is 45 to 54\",\n",
    "        5: \"my age is 55 to 64\",\n",
    "        6: \"my age is 65 or older\",\n",
    "    },\n",
    "\n",
    "    '_SEX': {\n",
    "        1: \"Male\",\n",
    "        2: \"Female\"\n",
    "    },\n",
    "\n",
    "    '_RACE': {\n",
    "        1: \"White only, non-Hispanic\",\n",
    "        2: \"Black only, non-Hispanic\",\n",
    "        3: \"American Indian or Alaskan Native only, Non-Hispanic\",\n",
    "        4: \"Asian only, non-Hispanic\",\n",
    "        5: \"Native Hawaiian or other Pacific Islander only, Non-Hispanic\",\n",
    "        6: \"Other race only, non-Hispanic\",\n",
    "        7: \"Multiracial, non-Hispanic\",\n",
    "        8: \"Hispanic\"\n",
    "    }\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target Variable: '_MICHD': {\n",
    "        1: \"Reported having MI or CHD\",\n",
    "        2: \"Did not report having MI or CHD\"\n",
    "    },"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Text Dataset Creation\n",
    "We create text datasets based on the transformation of the numerical values to string/text values. Two datasets will be created:\n",
    "- Dataset A: This dataset will transform each variable value into a small sentence\n",
    "- Dataset B: This dataset will transform each variable value into its corresponding text value, according to the text values provided in \"2021 SMART BRFSS Survey Codebook\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following algorithm implements the generation of text sequences from the original data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create an empty list to store rows for the two datasets\n",
    "new_dataset_rows = [\n",
    "    [],\n",
    "    []\n",
    "]\n",
    "\n",
    "# Create an empty list to store the two datasets\n",
    "text_data = []\n",
    "\n",
    "# Iterate through each row in the original dataset\n",
    "for _, row in SMOTE_data.iterrows():\n",
    "    sentence_parts = [\n",
    "        [],  # To store parts of the sentence for each row for dataset A\n",
    "        []   # To store parts of the sentence for each row for dataset B\n",
    "    ]\n",
    "\n",
    "    # Iterate through each column in the original dataset\n",
    "    for column_name, value in row.items():\n",
    "        # Check if the current column is in the sequences dictionary. Both the sequences and the string_var_values dictionaries have the same keys.\n",
    "        if (column_name in sequences) and (value in sequences[column_name]):\n",
    "\n",
    "            # === Dataset A ===\n",
    "            # Get the description for the current value of the variable\n",
    "            description = sequences[column_name].get(value, '')\n",
    "            # Add a comma to the end of the description\n",
    "            description += ','\n",
    "            # Add the description to the sentence parts for dataset A\n",
    "            sentence_parts[0].append(description)\n",
    "\n",
    "            # === Dataset B ===\n",
    "            # Get the description for the current value of the variable\n",
    "            description = string_var_values[column_name].get(value, '')\n",
    "            # Add a comma to the end of the description\n",
    "            description += ','\n",
    "            # Add the description to the sentence parts for dataset B\n",
    "            sentence_parts[1].append(description)\n",
    "        \n",
    "        # Check if the current column is the target variable\n",
    "        elif column_name == '_MICHD':\n",
    "            # Add the target variable's value to the second column of both datasets\n",
    "            classification_value = value\n",
    "    \n",
    "    # === Dataset A ===\n",
    "    # Concatenate the sentence parts into a single sentence\n",
    "    sentence = ' '.join(sentence_parts[0])\n",
    "    \n",
    "    # Create a new row for dataset A\n",
    "    new_dataset_rows[0].append({'Sentence': sentence, 'Classification': classification_value})\n",
    "\n",
    "    # === Dataset B ===\n",
    "    # Concatenate the sentence parts into a single sentence \n",
    "    sentence = ' '.join(sentence_parts[1])\n",
    "    \n",
    "    # Create a new row for dataset B\n",
    "    new_dataset_rows[1].append({'Sentence': sentence, 'Classification': classification_value})\n",
    "\n",
    "# Create the new dataset from the list of rows\n",
    "text_data.append(pd.DataFrame(new_dataset_rows[0]))\n",
    "text_data.append(pd.DataFrame(new_dataset_rows[1]))\n",
    "\n",
    "# Display dataset A\n",
    "print(text_data[0])\n",
    "\n",
    "print()\n",
    "\n",
    "# Display dataset B\n",
    "print(text_data[1])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Capitalize the \"i\" in the beginning of each sentence, remove the comma from the end fo the sentence and replace it with a period.\n",
    "text_data[0]['Sentence'] = text_data[0]['Sentence'].str.rstrip(',')  # Remove comma from the end\n",
    "text_data[0]['Sentence'] = text_data[0]['Sentence'].str.capitalize()  # Capitalize \"I\" at the beginning\n",
    "text_data[0]['Sentence'] = text_data[0]['Sentence'].str.replace(r'\\bi\\b', 'I')  # Capitalize standalone \"i\"\n",
    "text_data[0]['Sentence'] = text_data[0]['Sentence'] + '.'  # Replace comma with a period at the end"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(text_data[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "text_data[1]['Sentence'] = text_data[1]['Sentence'].str.rstrip(',')  # Remove comma from the end"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Based on the value (custom sentence, string/text), convert the \"_MICHD\" target variable values to 1 and 0\n",
    "text_data['Classification'] = text_data['Classification'].map({'Yes': 1, 'No': 0})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "text_data[0]['Classification'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "text_data[1]['Classification'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save the new sentence dataset. This dataset will be used for sequence classification tasks.\n",
    "text_data[1].to_excel(\"Text_Sequence_Dataset.xlsx\",  index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using Multinomial Naive Bayes Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# We can add the \"NOT_\" word infront of words that come after negation words\n",
    "def add_not_to_words(sentence):\n",
    "    words = sentence.split()\n",
    "    new_sentence = []\n",
    "\n",
    "    add_not = False\n",
    "\n",
    "    for word in words:\n",
    "        if add_not:\n",
    "            new_sentence.append(\"NOT_\" + word)\n",
    "            # Stop adding \"NOT_\" when a comma is encountered\n",
    "            if \",\" in word:\n",
    "                add_not = False\n",
    "        else:\n",
    "            new_sentence.append(word)\n",
    "\n",
    "        if word.lower() in ['not', \"didn't\", \"don't\", \"haven't\", \"never\"]:\n",
    "            add_not = True\n",
    "\n",
    "    return ' '.join(new_sentence)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "negation_text_data = text_data[1].copy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Apply the function to every sentence in the DataFrame\n",
    "negation_text_data['Sentence'] = negation_text_data['Sentence'].apply(add_not_to_words)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial NB\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split the dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(negation_text_data['Sentence'], negation_text_data['Classification'], test_size=0.25, random_state=SEED, stratify=negation_text_data['Classification'])\n",
    "\n",
    "# Create a CountVectorizer to convert the text data into numerical features\n",
    "vectorizer = CountVectorizer()\n",
    "x_train_vectorized = vectorizer.fit_transform(x_train)\n",
    "x_test_vectorized = vectorizer.transform(x_test)\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train_vectorized, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(x_test_vectorized)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred).T\n",
    "print(f'Confusion Matrix: \\n{conf_matrix}\\n')\n",
    "\n",
    "# Calculate the total cost\n",
    "total_cost = np.sum(conf_matrix * cost_matrix)\n",
    "print(f'Total Cost: {total_cost}\\n')\n",
    "\n",
    "# Evaluate the performance\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Display the results\n",
    "print('Classification Report:')\n",
    "print(classification_rep)\n",
    "\n",
    "# Extract macro metrics from the classification report\n",
    "metrics_dict = classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"], output_dict=True)['macro avg']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Read the existing CSV file\n",
    "# df = pd.read_csv('SMOTE/SMOTE_f1_score.csv')\n",
    "# df.at[0, 'Multinomial NB'] = metrics_dict['f1-score']\n",
    "# df.to_csv('SMOTE/SMOTE_f1_score.csv', index=False)\n",
    "# \n",
    "# # Read the existing CSV file\n",
    "# df1 = pd.read_csv('SMOTE/SMOTE_loss.csv')\n",
    "# # Add data to the new column in the specific line\n",
    "# df1.at[0, 'Multinomial NB'] = total_cost\n",
    "# df1.to_csv('SMOTE/SMOTE_loss.csv', index=False)\n",
    "\n",
    "# Read the existing CSV file\n",
    "df2 = pd.read_csv('SMOTE/SMOTE_precision.csv')\n",
    "# Add data to the new column in the specific line\n",
    "df2.at[0, 'Multinomial NB'] = metrics_dict['precision']\n",
    "df2.to_csv('SMOTE/SMOTE_precision.csv', index=False)\n",
    "\n",
    "# # Read the existing CSV file\n",
    "# df3 = pd.read_csv('SMOTE/SMOTE_recall.csv')\n",
    "# df3.at[0, 'Multinomial NB'] = metrics_dict['recall']\n",
    "# df3.to_csv('SMOTE/SMOTE_recall.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get predicted probabilities for the positive class\n",
    "y_prob = clf.predict_proba(x_test_vectorized)[:, 1]\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Random Guessing')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Save the plot\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split the dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(negation_text_data['Sentence'], negation_text_data['Classification'], test_size=0.25, random_state=SEED, stratify=negation_text_data['Classification'])\n",
    "\n",
    "def predict(y_prob, cost_m):\n",
    "    t_BMR = (cost_m[1][0] - cost_m[1][1])/(cost_m[0][1] - cost_m[1][1] - cost_m[0][0] + cost_m[1][0])\n",
    "    y_pred = np.greater(y_prob[:, 1], t_BMR).astype(int)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Create a CountVectorizer to convert the text data into numerical features\n",
    "vectorizer = CountVectorizer()\n",
    "x_train_vectorized = vectorizer.fit_transform(x_train)\n",
    "x_test_vectorized = vectorizer.transform(x_test)\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier\n",
    "clf = CalibratedClassifierCV(MultinomialNB(),\n",
    "                                method=\"isotonic\",\n",
    "                                cv=10\n",
    "                            )\n",
    "clf.fit(x_train_vectorized, y_train)\n",
    "\n",
    "y_prob = clf.predict_proba(x_test_vectorized)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = predict(y_prob, cost_matrix)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred).T\n",
    "print(f'Confusion Matrix: \\n{conf_matrix}\\n')\n",
    "\n",
    "# Calculate the total cost\n",
    "total_cost = np.sum(conf_matrix * cost_matrix)\n",
    "print(f'Total Cost: {total_cost}\\n')\n",
    "\n",
    "# Evaluate the performance\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Display the results\n",
    "print('Classification Report:')\n",
    "print(classification_rep)\n",
    "\n",
    "# Extract macro metrics from the classification report\n",
    "metrics_dict = classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"], output_dict=True)['macro avg']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Read the existing CSV file\n",
    "# df = pd.read_csv('SMOTE/SMOTE_f1_score.csv')\n",
    "# df.at[0, 'Multinomial NB'] = metrics_dict['f1-score']\n",
    "# df.to_csv('SMOTE/SMOTE_f1_score.csv', index=False)\n",
    "# \n",
    "# # Read the existing CSV file\n",
    "# df1 = pd.read_csv('SMOTE/SMOTE_loss.csv')\n",
    "# # Add data to the new column in the specific line\n",
    "# df1.at[0, 'Multinomial NB'] = total_cost\n",
    "# df1.to_csv('SMOTE/SMOTE_loss.csv', index=False)\n",
    "\n",
    "# Read the existing CSV file\n",
    "df2 = pd.read_csv('SMOTE/SMOTE_precision.csv')\n",
    "# Add data to the new column in the specific line\n",
    "df2.at[1, 'Multinomial NB'] = metrics_dict['precision']\n",
    "df2.to_csv('SMOTE/SMOTE_precision.csv', index=False)\n",
    "\n",
    "# # Read the existing CSV file\n",
    "# df3 = pd.read_csv('SMOTE/SMOTE_recall.csv')\n",
    "# df3.at[0, 'Multinomial NB'] = metrics_dict['recall']\n",
    "# df3.to_csv('SMOTE/SMOTE_recall.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Get predicted probabilities for the positive class\n",
    "y_pred_proba = clf.predict_proba(x_test_vectorized)[:, 1]\n",
    "\n",
    "# Compute calibration curve\n",
    "prob_true, prob_pred = calibration_curve(y_test, y_pred_proba, n_bins=10)\n",
    "\n",
    "# Plot calibration curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(prob_pred, prob_true, marker='o', linestyle='-')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Ideal curve\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.title('Probability Calibration Curve')\n",
    "\n",
    "# Save the plot as an image file (e.g., PNG)\n",
    "\n",
    "# Show the plot (optional)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Split the dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(negation_text_data['Sentence'], negation_text_data['Classification'], test_size=0.25, random_state=SEED, stratify=negation_text_data['Classification'])\n",
    "\n",
    "weights = np.zeros_like(y_train, dtype=float)\n",
    "weights[y_train == 0] = cost_matrix[0][1]\n",
    "weights[y_train == 1] = cost_matrix[1][0]\n",
    "\n",
    "# Create a CountVectorizer to convert the text data into numerical features\n",
    "vectorizer = CountVectorizer()\n",
    "x_train_vectorized = vectorizer.fit_transform(x_train)\n",
    "x_test_vectorized = vectorizer.transform(x_test)\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train_vectorized, y_train, sample_weight=weights)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(x_test_vectorized)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred).T\n",
    "print(f'Confusion Matrix: \\n{conf_matrix}\\n')\n",
    "\n",
    "# Calculate the total cost\n",
    "total_cost = np.sum(conf_matrix * cost_matrix)\n",
    "print(f'Total Cost: {total_cost}\\n')\n",
    "\n",
    "# Evaluate the performance\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Display the results\n",
    "print('Classification Report:')\n",
    "print(classification_rep)\n",
    "\n",
    "# Extract macro metrics from the classification report\n",
    "metrics_dict = classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"], output_dict=True)['macro avg']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Read the existing CSV file\n",
    "# df = pd.read_csv('SMOTE/SMOTE_f1_score.csv')\n",
    "# df.at[0, 'Multinomial NB'] = metrics_dict['f1-score']\n",
    "# df.to_csv('SMOTE/SMOTE_f1_score.csv', index=False)\n",
    "# \n",
    "# # Read the existing CSV file\n",
    "# df1 = pd.read_csv('SMOTE/SMOTE_loss.csv')\n",
    "# # Add data to the new column in the specific line\n",
    "# df1.at[0, 'Multinomial NB'] = total_cost\n",
    "# df1.to_csv('SMOTE/SMOTE_loss.csv', index=False)\n",
    "\n",
    "# Read the existing CSV file\n",
    "df2 = pd.read_csv('SMOTE/SMOTE_precision.csv')\n",
    "# Add data to the new column in the specific line\n",
    "df2.at[2, 'Multinomial NB'] = metrics_dict['precision']\n",
    "df2.to_csv('SMOTE/SMOTE_precision.csv', index=False)\n",
    "\n",
    "# # Read the existing CSV file\n",
    "# df3 = pd.read_csv('SMOTE/SMOTE_recall.csv')\n",
    "# df3.at[0, 'Multinomial NB'] = metrics_dict['recall']\n",
    "# df3.to_csv('SMOTE/SMOTE_recall.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-variate Bernoulli NB\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split the dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(negation_text_data['Sentence'], negation_text_data['Classification'], test_size=0.25, random_state=SEED, stratify=negation_text_data['Classification'])\n",
    "\n",
    "# Create a CountVectorizer to convert the text data into numerical features\n",
    "vectorizer = CountVectorizer()\n",
    "x_train_vectorized = vectorizer.fit_transform(x_train)\n",
    "x_test_vectorized = vectorizer.transform(x_test)\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier\n",
    "clf = BernoulliNB()\n",
    "clf.fit(x_train_vectorized, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(x_test_vectorized)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred).T\n",
    "print(f'Confusion Matrix: \\n{conf_matrix}\\n')\n",
    "\n",
    "# Calculate the total cost\n",
    "total_cost = np.sum(conf_matrix * cost_matrix)\n",
    "print(f'Total Cost: {total_cost}\\n')\n",
    "\n",
    "# Evaluate the performance\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Display the results\n",
    "print('Classification Report:')\n",
    "print(classification_rep)\n",
    "\n",
    "# Extract macro metrics from the classification report\n",
    "metrics_dict = classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"], output_dict=True)['macro avg']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Read the existing CSV file\n",
    "# df = pd.read_csv('SMOTE/SMOTE_f1_score.csv')\n",
    "# df.at[0, 'Multi-variate Bernoulli NB'] = metrics_dict['f1-score']\n",
    "# df.to_csv('SMOTE/SMOTE_f1_score.csv', index=False)\n",
    "# \n",
    "# # Read the existing CSV file\n",
    "# df1 = pd.read_csv('SMOTE/SMOTE_loss.csv')\n",
    "# # Add data to the new column in the specific line\n",
    "# df1.at[0, 'Multi-variate Bernoulli NB'] = total_cost\n",
    "# df1.to_csv('SMOTE/SMOTE_loss.csv', index=False)\n",
    "\n",
    "# Read the existing CSV file\n",
    "df2 = pd.read_csv('SMOTE/SMOTE_precision.csv')\n",
    "# Add data to the new column in the specific line\n",
    "df2.at[0, 'Multi-variate Bernoulli NB'] = metrics_dict['precision']\n",
    "df2.to_csv('SMOTE/SMOTE_precision.csv', index=False)\n",
    "\n",
    "# # Read the existing CSV file\n",
    "# df3 = pd.read_csv('SMOTE/SMOTE_recall.csv')\n",
    "# df3.at[0, 'Multi-variate Bernoulli NB'] = metrics_dict['recall']\n",
    "# df3.to_csv('SMOTE/SMOTE_recall.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split the dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(negation_text_data['Sentence'], negation_text_data['Classification'], test_size=0.25, random_state=SEED, stratify=negation_text_data['Classification'])\n",
    "\n",
    "def predict(y_prob, cost_m):\n",
    "    t_BMR = (cost_m[1][0] - cost_m[1][1])/(cost_m[0][1] - cost_m[1][1] - cost_m[0][0] + cost_m[1][0])\n",
    "    y_pred = np.greater(y_prob[:, 1], t_BMR).astype(int)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Create a CountVectorizer to convert the text data into numerical features\n",
    "vectorizer = CountVectorizer()\n",
    "x_train_vectorized = vectorizer.fit_transform(x_train)\n",
    "x_test_vectorized = vectorizer.transform(x_test)\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier\n",
    "clf = CalibratedClassifierCV(BernoulliNB(),\n",
    "                            method=\"isotonic\",\n",
    "                            cv=10)\n",
    "clf.fit(x_train_vectorized, y_train)\n",
    "\n",
    "y_prob = clf.predict_proba(x_test_vectorized)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = predict(y_prob, cost_matrix)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred).T\n",
    "print(f'Confusion Matrix: \\n{conf_matrix}\\n')\n",
    "\n",
    "# Calculate the total cost\n",
    "total_cost = np.sum(conf_matrix * cost_matrix)\n",
    "print(f'Total Cost: {total_cost}\\n')\n",
    "\n",
    "# Evaluate the performance\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Display the results\n",
    "print('Classification Report:')\n",
    "print(classification_rep)\n",
    "\n",
    "# Extract macro metrics from the classification report\n",
    "metrics_dict = classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"], output_dict=True)['macro avg']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Read the existing CSV file\n",
    "# df = pd.read_csv('SMOTE/SMOTE_f1_score.csv')\n",
    "# df.at[0, 'Multi-variate Bernoulli NB'] = metrics_dict['f1-score']\n",
    "# df.to_csv('SMOTE/SMOTE_f1_score.csv', index=False)\n",
    "# \n",
    "# # Read the existing CSV file\n",
    "# df1 = pd.read_csv('SMOTE/SMOTE_loss.csv')\n",
    "# # Add data to the new column in the specific line\n",
    "# df1.at[0, 'Multi-variate Bernoulli NB'] = total_cost\n",
    "# df1.to_csv('SMOTE/SMOTE_loss.csv', index=False)\n",
    "\n",
    "# Read the existing CSV file\n",
    "df2 = pd.read_csv('SMOTE/SMOTE_precision.csv')\n",
    "# Add data to the new column in the specific line\n",
    "df2.at[1, 'Multi-variate Bernoulli NB'] = metrics_dict['precision']\n",
    "df2.to_csv('SMOTE/SMOTE_precision.csv', index=False)\n",
    "\n",
    "# # Read the existing CSV file\n",
    "# df3 = pd.read_csv('SMOTE/SMOTE_recall.csv')\n",
    "# df3.at[0, 'Multi-variate Bernoulli NB'] = metrics_dict['recall']\n",
    "# df3.to_csv('SMOTE/SMOTE_recall.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Split the dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(negation_text_data['Sentence'], negation_text_data['Classification'], test_size=0.25, random_state=SEED, stratify=negation_text_data['Classification'])\n",
    "\n",
    "weights = np.zeros_like(y_train, dtype=float)\n",
    "weights[y_train == 0] = cost_matrix[0][1]\n",
    "weights[y_train == 1] = cost_matrix[1][0]\n",
    "\n",
    "# Create a CountVectorizer to convert the text data into numerical features\n",
    "vectorizer = CountVectorizer()\n",
    "x_train_vectorized = vectorizer.fit_transform(x_train)\n",
    "x_test_vectorized = vectorizer.transform(x_test)\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier\n",
    "clf = BernoulliNB()\n",
    "clf.fit(x_train_vectorized, y_train, sample_weight=weights)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(x_test_vectorized)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred).T\n",
    "print(f'Confusion Matrix: \\n{conf_matrix}\\n')\n",
    "\n",
    "# Calculate the total cost\n",
    "total_cost = np.sum(conf_matrix * cost_matrix)\n",
    "print(f'Total Cost: {total_cost}\\n')\n",
    "\n",
    "# Evaluate the performance\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Display the results\n",
    "print('Classification Report:')\n",
    "print(classification_rep)\n",
    "\n",
    "# Extract macro metrics from the classification report\n",
    "metrics_dict = classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"], output_dict=True)['macro avg']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Read the existing CSV file\n",
    "# df = pd.read_csv('SMOTE/SMOTE_f1_score.csv')\n",
    "# df.at[0, 'Multi-variate Bernoulli NB'] = metrics_dict['f1-score']\n",
    "# df.to_csv('SMOTE/SMOTE_f1_score.csv', index=False)\n",
    "# \n",
    "# # Read the existing CSV file\n",
    "# df1 = pd.read_csv('SMOTE/SMOTE_loss.csv')\n",
    "# # Add data to the new column in the specific line\n",
    "# df1.at[0, 'Multi-variate Bernoulli NB'] = total_cost\n",
    "# df1.to_csv('SMOTE/SMOTE_loss.csv', index=False)\n",
    "\n",
    "# Read the existing CSV file\n",
    "df2 = pd.read_csv('SMOTE/SMOTE_precision.csv')\n",
    "# Add data to the new column in the specific line\n",
    "df2.at[2, 'Multi-variate Bernoulli NB'] = metrics_dict['precision']\n",
    "df2.to_csv('SMOTE/SMOTE_precision.csv', index=False)\n",
    "\n",
    "# # Read the existing CSV file\n",
    "# df3 = pd.read_csv('SMOTE/SMOTE_recall.csv')\n",
    "# df3.at[0, 'Multi-variate Bernoulli NB'] = metrics_dict['recall']\n",
    "# df3.to_csv('SMOTE/SMOTE_recall.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Binary Multinomial NB"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split the dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(negation_text_data['Sentence'], negation_text_data['Classification'], test_size=0.25, random_state=SEED, stratify=negation_text_data['Classification'])\n",
    "\n",
    "# Create a CountVectorizer to convert the text data into numerical features\n",
    "vectorizer = CountVectorizer()\n",
    "x_train_vectorized = vectorizer.fit_transform(x_train.apply(lambda x: ' '.join(set(x.split()))))\n",
    "x_test_vectorized = vectorizer.transform(x_test.apply(lambda x: ' '.join(set(x.split()))))\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train_vectorized, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(x_test_vectorized)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred).T\n",
    "print(f'Confusion Matrix: \\n{conf_matrix}\\n')\n",
    "\n",
    "# Calculate the total cost\n",
    "total_cost = np.sum(conf_matrix * cost_matrix)\n",
    "print(f'Total Cost: {total_cost}\\n')\n",
    "\n",
    "# Evaluate the performance\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Display the results\n",
    "print('Classification Report:')\n",
    "print(classification_rep)\n",
    "\n",
    "# Extract macro metrics from the classification report\n",
    "metrics_dict = classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"], output_dict=True)['macro avg']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Read the existing CSV file\n",
    "# df = pd.read_csv('SMOTE/SMOTE_f1_score.csv')\n",
    "# df.at[0, 'Binary Multinomial NB'] = metrics_dict['f1-score']\n",
    "# df.to_csv('SMOTE/SMOTE_f1_score.csv', index=False)\n",
    "# \n",
    "# # Read the existing CSV file\n",
    "# df1 = pd.read_csv('SMOTE/SMOTE_loss.csv')\n",
    "# # Add data to the new column in the specific line\n",
    "# df1.at[0, 'Binary Multinomial NB'] = total_cost\n",
    "# df1.to_csv('SMOTE/SMOTE_loss.csv', index=False)\n",
    "\n",
    "# Read the existing CSV file\n",
    "df2 = pd.read_csv('SMOTE/SMOTE_precision.csv')\n",
    "# Add data to the new column in the specific line\n",
    "df2.at[0, 'Binary Multinomial NB'] = metrics_dict['precision']\n",
    "df2.to_csv('SMOTE/SMOTE_precision.csv', index=False)\n",
    "\n",
    "# # Read the existing CSV file\n",
    "# df3 = pd.read_csv('SMOTE/SMOTE_recall.csv')\n",
    "# df3.at[0, 'Binary Multinomial NB'] = metrics_dict['recall']\n",
    "# df3.to_csv('SMOTE/SMOTE_recall.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split the dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(negation_text_data['Sentence'], negation_text_data['Classification'], test_size=0.25, random_state=SEED, stratify=negation_text_data['Classification'])\n",
    "\n",
    "def predict(y_prob, cost_m):\n",
    "    t_BMR = (cost_m[1][0] - cost_m[1][1])/(cost_m[0][1] - cost_m[1][1] - cost_m[0][0] + cost_m[1][0])\n",
    "    y_pred = np.greater(y_prob[:, 1], t_BMR).astype(int)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Create a CountVectorizer to convert the text data into numerical features\n",
    "vectorizer = CountVectorizer()\n",
    "x_train_vectorized = vectorizer.fit_transform(x_train.apply(lambda x: ' '.join(set(x.split()))))\n",
    "x_test_vectorized = vectorizer.transform(x_test.apply(lambda x: ' '.join(set(x.split()))))\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier\n",
    "clf = CalibratedClassifierCV(MultinomialNB(),\n",
    "                            method=\"isotonic\",\n",
    "                            cv=10)\n",
    "clf.fit(x_train_vectorized, y_train)\n",
    "\n",
    "y_prob = clf.predict_proba(x_test_vectorized)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = predict(y_prob, cost_matrix)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred).T\n",
    "print(f'Confusion Matrix: \\n{conf_matrix}\\n')\n",
    "\n",
    "# Calculate the total cost\n",
    "total_cost = np.sum(conf_matrix * cost_matrix)\n",
    "print(f'Total Cost: {total_cost}\\n')\n",
    "\n",
    "# Evaluate the performance\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Display the results\n",
    "print('Classification Report:')\n",
    "print(classification_rep)\n",
    "\n",
    "# Extract macro metrics from the classification report\n",
    "metrics_dict = classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"], output_dict=True)['macro avg']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Read the existing CSV file\n",
    "# df = pd.read_csv('SMOTE/SMOTE_f1_score.csv')\n",
    "# df.at[0, 'Binary Multinomial NB'] = metrics_dict['f1-score']\n",
    "# df.to_csv('SMOTE/SMOTE_f1_score.csv', index=False)\n",
    "# \n",
    "# # Read the existing CSV file\n",
    "# df1 = pd.read_csv('SMOTE/SMOTE_loss.csv')\n",
    "# # Add data to the new column in the specific line\n",
    "# df1.at[0, 'Binary Multinomial NB'] = total_cost\n",
    "# df1.to_csv('SMOTE/SMOTE_loss.csv', index=False)\n",
    "\n",
    "# Read the existing CSV file\n",
    "df2 = pd.read_csv('SMOTE/SMOTE_precision.csv')\n",
    "# Add data to the new column in the specific line\n",
    "df2.at[1, 'Binary Multinomial NB'] = metrics_dict['precision']\n",
    "df2.to_csv('SMOTE/SMOTE_precision.csv', index=False)\n",
    "\n",
    "# # Read the existing CSV file\n",
    "# df3 = pd.read_csv('SMOTE/SMOTE_recall.csv')\n",
    "# df3.at[0, 'Binary Multinomial NB'] = metrics_dict['recall']\n",
    "# df3.to_csv('SMOTE/SMOTE_recall.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Split the dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(negation_text_data['Sentence'], negation_text_data['Classification'], test_size=0.25, random_state=SEED, stratify=negation_text_data['Classification'])\n",
    "\n",
    "weights = np.zeros_like(y_train, dtype=float)\n",
    "weights[y_train == 0] = cost_matrix[0][1]\n",
    "weights[y_train == 1] = cost_matrix[1][0]\n",
    "\n",
    "# Create a CountVectorizer to convert the text data into numerical features\n",
    "vectorizer = CountVectorizer()\n",
    "x_train_vectorized = vectorizer.fit_transform(x_train.apply(lambda x: ' '.join(set(x.split()))))\n",
    "x_test_vectorized = vectorizer.transform(x_test.apply(lambda x: ' '.join(set(x.split()))))\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train_vectorized, y_train, sample_weight=weights)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(x_test_vectorized)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred).T\n",
    "print(f'Confusion Matrix: \\n{conf_matrix}\\n')\n",
    "\n",
    "# Calculate the total cost\n",
    "total_cost = np.sum(conf_matrix * cost_matrix)\n",
    "print(f'Total Cost: {total_cost}\\n')\n",
    "\n",
    "# Evaluate the performance\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Display the results\n",
    "print('Classification Report:')\n",
    "print(classification_rep)\n",
    "\n",
    "# Extract macro metrics from the classification report\n",
    "metrics_dict = classification_report(y_test, y_pred, target_names=[\"Reported having MI or CHD\", \"Did not report having MI or CHD\"], output_dict=True)['macro avg']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Read the existing CSV file\n",
    "# df = pd.read_csv('SMOTE/SMOTE_f1_score.csv')\n",
    "# df.at[0, 'Binary Multinomial NB'] = metrics_dict['f1-score']\n",
    "# df.to_csv('SMOTE/SMOTE_f1_score.csv', index=False)\n",
    "# \n",
    "# # Read the existing CSV file\n",
    "# df1 = pd.read_csv('SMOTE/SMOTE_loss.csv')\n",
    "# # Add data to the new column in the specific line\n",
    "# df1.at[0, 'Binary Multinomial NB'] = total_cost\n",
    "# df1.to_csv('SMOTE/SMOTE_loss.csv', index=False)\n",
    "\n",
    "# Read the existing CSV file\n",
    "df2 = pd.read_csv('SMOTE/SMOTE_precision.csv')\n",
    "# Add data to the new column in the specific line\n",
    "df2.at[2, 'Binary Multinomial NB'] = metrics_dict['precision']\n",
    "df2.to_csv('SMOTE/SMOTE_precision.csv', index=False)\n",
    "\n",
    "# # Read the existing CSV file\n",
    "# df3 = pd.read_csv('SMOTE/SMOTE_recall.csv')\n",
    "# df3.at[0, 'Binary Multinomial NB'] = metrics_dict['recall']\n",
    "# df3.to_csv('SMOTE/SMOTE_recall.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CDC-BRFSS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
